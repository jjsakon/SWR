{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import all necessary modules\n",
    "import sys\n",
    "# sys.path.append('/home1/esolo/ptsa_new/')\n",
    "# sys.path.append('/home1/mran/PTSA_GIT/')\n",
    "import numpy.testing\n",
    "import numpy as np\n",
    "# from ptsa.data.readers.IndexReader import JsonIndexReader # 08/30/17: Added for json file format\n",
    "from ptsa.data.readers import BaseEventReader\n",
    "# from ptsa.data.readers.TalReader import TalReader\n",
    "from ptsa.data.readers import EEGReader\n",
    "from ptsa.data.filters import MonopolarToBipolarMapper\n",
    "from ptsa.data.filters import ButterworthFilter\n",
    "# from ptsa.data.filters.MorletWaveletFilter import MorletWaveletFilter\n",
    "from ptsa.data.filters import ResampleFilter\n",
    "# from ptsa.data.common import xr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from scipy.stats.mstats import zscore\n",
    "from time import time\n",
    "start_time = time()\n",
    "\n",
    "#Added modules \n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Find where site packages are located\n",
    "# import site; site.getsitepackages()\n",
    "\n",
    "# Unix qlogin command:\n",
    "# qlogin -l h_vmem=8G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from Electrophysiology_Functions import * # This will import the python file of the electrophysiology functions (NOT ALL FUNCTIONS PRESENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Import plotting modules\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# FUNCTION OUTDATED: No longer use .mat files\n",
    "def get_all_subs_math(taskname, rhino_prefix='/'):\n",
    "    from glob import glob\n",
    "    \n",
    "    fns = glob(rhino_prefix+'/data/events/'+taskname+'/*_math.mat')\n",
    "    subs = [s.split('/')[-1].split('_m')[0] for s in fns]\n",
    "    \n",
    "    return(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ReadSubjBaseEvents_Json(study_name, subject):\n",
    "    ### Read in subjects events and electrode positions from the Json files \n",
    "    ### Updated: 8/25/17 - technically functional, poorly written\n",
    "  \n",
    "    # Work around for study_name changes from my old code to json experiment names\n",
    "    if study_name == 'RAM_FR1':\n",
    "        experiment = 'FR1'\n",
    "    elif study_name == 'RAM_CatFR1':\n",
    "        experiment = 'catFR1'\n",
    "    \n",
    "    if study_name == 'pyFR':\n",
    "        # Old pathway with .mat files\n",
    "        e_path = '/data/events/'+study_name+'/'+subject+'_math.mat'\n",
    "        tal_path = '/data/eeg/'+subject+'/tal/'+subject+'_talLocs_database_bipol.mat'\n",
    "\n",
    "        # ------------------- READING EVENTS\n",
    "        base_e_reader = BaseEventReader(filename=e_path, eliminate_events_with_no_eeg=True)\n",
    "        base_events = base_e_reader.read()\n",
    "        base_events = base_events[base_events.type == 'PROB']\n",
    "\n",
    "        # ------------------- READING TAL STRUCTS\n",
    "        tal_reader = TalReader(filename=tal_path)\n",
    "        monopolar_channels = tal_reader.get_monopolar_channels()\n",
    "        bipolar_pairs = tal_reader.get_bipolar_pairs()\n",
    "\n",
    "    elif study_name == 'RAM_FR1_CatFR1':\n",
    "        if subject[-2] == '_': # Detects if subject has multiple montages\n",
    "            s_montage = subject # Keeps the montage number on the subject name i.e. '_1'\n",
    "            montage = subject[-1] # Delineate which montage will be used for this subject\n",
    "            subject = subject[:-2] # Redefine subject without montage specified\n",
    "            \n",
    "            ## Loads all events and then restricts to the specific montage\n",
    "            jr = JsonIndexReader('/protocols/r1.json')\n",
    "            \n",
    "            ## RAM_FR1 \n",
    "            experiment ='FR1'\n",
    "            basefile_path = list(sorted(jr.aggregate_values('math_events',subject=subject,experiment=experiment))) #Sorted to organize sessions in ascending order\n",
    "            base_events = []\n",
    "            for indx, session_path in enumerate(basefile_path): ## TEMP FIX: List order reversed to be in numerical session order\n",
    "                base_e_reader = BaseEventReader(filename=session_path, eliminate_events_with_no_eeg=True)\n",
    "                base_events.append(base_e_reader.read())\n",
    "\n",
    "            base_events = np.concatenate(base_events).view(np.recarray)\n",
    "            base_events = base_events[base_events.type == 'PROB']\n",
    "\n",
    "            event_montages = [int(x[2]) for x in base_events.montage] # Extracts montage numbers from the events\n",
    "            subject_montage = (np.ones(len(event_montages)) * int(montage)) # Creates an array for boolean comparison to the event montage\n",
    "            FR1_base_events = base_events[np.array(event_montages) == np.array(subject_montage)]\n",
    "\n",
    "            ## RAM_CatFR1 \n",
    "            experiment ='catFR1'\n",
    "            basefile_path = list(sorted(jr.aggregate_values('math_events',subject=subject,experiment=experiment)))\n",
    "            base_events = []\n",
    "            for indx, session_path in enumerate(basefile_path): ## TEMP FIX: List order reversed to be in numerical session order [::-1] (was the fix, may be the problem)\n",
    "                base_e_reader = BaseEventReader(filename=session_path, eliminate_events_with_no_eeg=True)\n",
    "                base_events.append(base_e_reader.read())\n",
    "\n",
    "            base_events = np.concatenate(base_events).view(np.recarray)\n",
    "            base_events = base_events[base_events.type == 'PROB']\n",
    "\n",
    "            event_montages = [int(x[2]) for x in base_events.montage] # Extracts montage numbers from the events\n",
    "            subject_montage = (np.ones(len(event_montages)) * int(montage)) # Creates an array for boolean comparison to the event montage\n",
    "            CatFR1_base_events = base_events[np.array(event_montages) == np.array(subject_montage)]\n",
    "\n",
    "            # Making the CatFR1 sessions have unique numbering (added as additional session to FR1)\n",
    "            CatFR1_base_events.session = CatFR1_base_events.session + max(np.unique(FR1_base_events.session))+1\n",
    "\n",
    "            # Combining the base_events where subjects completed both FR1 and CatFR1\n",
    "            base_events = np.hstack((FR1_base_events,CatFR1_base_events))\n",
    "\n",
    "            ## Loads specific electrode montage - for new json format\n",
    "#             pairs = list(jr.aggregate_values('pairs',subject=subject,experiment=experiment))[::-1]\n",
    "#             montage_pair = [int(x[55]) for x in pairs]\n",
    "#             from itertools import compress\n",
    "#             tal_path = list(compress(pairs,montage_pair == (np.ones(len(montage_pair)) * int(montage))))[0]\n",
    "            \n",
    "            # Loading tal_struct from old .mat files\n",
    "            if int(montage) != 0:\n",
    "                # Loads tal_path with montage specified\n",
    "                tal_path = '/data/eeg/'+s_montage+'/tal/'+s_montage+'_talLocs_database_bipol.mat' # Old tal_struct loading\n",
    "            else:\n",
    "                # Corrects since subjects are not labeled '_0'\n",
    "                tal_path = '/data/eeg/'+subject+'/tal/'+subject+'_talLocs_database_bipol.mat' # Old tal_struct loading\n",
    "\n",
    "            # Reading Tal Structs\n",
    "            tal_reader = TalReader(filename=tal_path)\n",
    "            monopolar_channels = tal_reader.get_monopolar_channels()\n",
    "            bipolar_pairs = tal_reader.get_bipolar_pairs()\n",
    "        \n",
    "        else:\n",
    "            # Reads in Json protocol file to extract file paths\n",
    "            jr = JsonIndexReader('/protocols/r1.json')\n",
    "\n",
    "            ## RAM_FR1\n",
    "            experiment = 'FR1'\n",
    "            FR1_basefile_path = list(sorted(jr.aggregate_values('math_events',subject=subject,experiment=experiment)))\n",
    "            FR1_base_events = []\n",
    "            for indx, session_path in enumerate(FR1_basefile_path): ## TEMP FIX: List order reversed to be in numerical session order\n",
    "                base_e_reader = BaseEventReader(filename=session_path, eliminate_events_with_no_eeg=True)\n",
    "                FR1_base_events.append(base_e_reader.read())\n",
    "            FR1_base_events = np.concatenate(FR1_base_events).view(np.recarray)\n",
    "            FR1_base_events = FR1_base_events[FR1_base_events.type == 'PROB']\n",
    "\n",
    "            ## RAM_CatFR1\n",
    "            experiment = 'catFR1'\n",
    "            CatFR1_basefile_path = list(sorted(jr.aggregate_values('math_events',subject=subject,experiment=experiment)))\n",
    "            CatFR1_base_events = []\n",
    "            for indx, session_path in enumerate(CatFR1_basefile_path): ## TEMP FIX: List order reversed to be in numerical session order\n",
    "                base_e_reader = BaseEventReader(filename=session_path, eliminate_events_with_no_eeg=True)\n",
    "                CatFR1_base_events.append(base_e_reader.read())\n",
    "            CatFR1_base_events = np.concatenate(CatFR1_base_events).view(np.recarray)\n",
    "            CatFR1_base_events = CatFR1_base_events[CatFR1_base_events.type == 'PROB']\n",
    "\n",
    "            # Making the CatFR1 sessions have unique numbering (added as additional session to FR1)\n",
    "            CatFR1_base_events.session = CatFR1_base_events.session + max(np.unique(FR1_base_events.session))+1\n",
    "\n",
    "            # Combining the base_events where subjects completed both FR1 and CatFR1\n",
    "            base_events = np.hstack((FR1_base_events,CatFR1_base_events))\n",
    "\n",
    "            ## Tal Path\n",
    "#             tal_path = jr.get_value('pairs',subject=subject,experiment=experiment) ## New .json format path for loading tal_struct - may need to use in the future but doesn't work now\n",
    "            #### Recovers tal_path for old .mat files using the montage number from json file\n",
    "            # Below finds if the montage is anything other than 0\n",
    "            pair = jr.get_value('pairs',subject=subject,experiment=experiment)\n",
    "            montage_pair = int(pair[55])\n",
    "            if montage_pair != 0:\n",
    "                subject = subject+'_'+str(montage_pair)\n",
    "            tal_path = '/data/eeg/'+subject+'/tal/'+subject+'_talLocs_database_bipol.mat' # Old tal_struct loading path\n",
    "\n",
    "            tal_reader = TalReader(filename=tal_path)\n",
    "            monopolar_channels = tal_reader.get_monopolar_channels()\n",
    "            bipolar_pairs = tal_reader.get_bipolar_pairs()\n",
    "    \n",
    "    else: \n",
    "        ## Loads data for specific subject with multiple montages\n",
    "        if subject[-2] == '_': # Detects if subject has multiple montages\n",
    "            s_montage = subject # Keeps the montage number on the subject name i.e. '_1'\n",
    "            montage = subject[-1] # Delineate which montage will be used for this subject\n",
    "            subject = subject[:-2] # Redefine subject without montage specified\n",
    "            \n",
    "            ## Loads all events and then restricts to the specific montage\n",
    "            jr = JsonIndexReader('/protocols/r1.json')\n",
    "            basefile_path = list(sorted(jr.aggregate_values('math_events',subject=subject,experiment=experiment)))\n",
    "            base_events = []\n",
    "            for indx, session_path in enumerate(basefile_path): ## TEMP FIX: List order reversed to be in numerical session order\n",
    "                base_e_reader = BaseEventReader(filename=session_path, eliminate_events_with_no_eeg=True)\n",
    "                base_events.append(base_e_reader.read())\n",
    "\n",
    "            base_events = np.concatenate(base_events).view(np.recarray)\n",
    "            base_events = base_events[base_events.type == 'PROB']\n",
    "            \n",
    "            event_montages = [int(x[2]) for x in base_events.montage] # Extracts montage numbers from the events\n",
    "            subject_montage = (np.ones(len(event_montages)) * int(montage)) # Creates an array for boolean comparison to the event montage\n",
    "            base_events = base_events[np.array(event_montages) == np.array(subject_montage)]\n",
    "                \n",
    "            ## Loads specific electrode montage - only new for new json format loading\n",
    "#             pairs = list(jr.aggregate_values('pairs',subject=subject,experiment=experiment))[::-1]\n",
    "#             montage_pair = [int(x[55]) for x in pairs]\n",
    "#             from itertools import compress\n",
    "#             tal_path = list(compress(pairs,montage_pair == (np.ones(len(montage_pair)) * int(montage))))[0]\n",
    "            \n",
    "            # Loading tal_struct from old .mat files\n",
    "            if int(montage) != 0:\n",
    "                # Loads tal_path with montage specified\n",
    "                tal_path = '/data/eeg/'+s_montage+'/tal/'+s_montage+'_talLocs_database_bipol.mat' # Old tal_struct loading\n",
    "            else:\n",
    "                # Corrects since subjects are not labeled '_0'\n",
    "                tal_path = '/data/eeg/'+subject+'/tal/'+subject+'_talLocs_database_bipol.mat' # Old tal_struct loading\n",
    "  \n",
    "            # Reading Tal Structs\n",
    "            tal_reader = TalReader(filename=tal_path)\n",
    "            monopolar_channels = tal_reader.get_monopolar_channels()\n",
    "            bipolar_pairs = tal_reader.get_bipolar_pairs()\n",
    "        else: \n",
    "            ## Regular pathway\n",
    "            try: \n",
    "                # Reads in Json protocol file to extract file paths\n",
    "                jr = JsonIndexReader('/protocols/r1.json')\n",
    "                # subject = jr.subjects(experiment='FR1')[12] # Way to read in subject manually (DON'T USE)\n",
    "\n",
    "                # Similar to e_path\n",
    "                basefile_path = list(sorted(jr.aggregate_values('math_events',subject=subject,experiment=experiment)))\n",
    "#                 tal_path = jr.get_value('pairs',subject=subject,experiment=experiment) # New json format, not using currently\n",
    "                \n",
    "                # Recovers old tal_path, making sure montage number is correct \n",
    "                pair = jr.get_value('pairs',subject=subject,experiment=experiment)\n",
    "                montage_pair = int(pair[55])\n",
    "                if montage_pair != 0:\n",
    "                    subject = subject+'_'+str(montage_pair)\n",
    "                tal_path = '/data/eeg/'+subject+'/tal/'+subject+'_talLocs_database_bipol.mat' # Old tal_struct loading path\n",
    "\n",
    "                base_events = []\n",
    "                for indx, session_path in enumerate(basefile_path): ## TEMP FIX: List order reversed to be in numerical session order\n",
    "                    base_e_reader = BaseEventReader(filename=session_path, eliminate_events_with_no_eeg=True)\n",
    "                    base_events.append(base_e_reader.read())\n",
    "\n",
    "                base_events = np.concatenate(base_events).view(np.recarray) # Converting back from list to perform recarray operations\n",
    "                base_events = base_events[base_events.type == 'PROB']\n",
    "\n",
    "                # Reading Tal Structs\n",
    "                tal_reader = TalReader(filename=tal_path)\n",
    "                monopolar_channels = tal_reader.get_monopolar_channels()\n",
    "                bipolar_pairs = tal_reader.get_bipolar_pairs()\n",
    "            except:\n",
    "                ### Attempts to find JSON format and then tries old method\n",
    "                e_path = '/data/events/'+study_name+'/'+subject+'_math.mat'\n",
    "                tal_path = '/data/eeg/'+subject+'/tal/'+subject+'_talLocs_database_bipol.mat'\n",
    "\n",
    "                # ------------------- READING EVENTS\n",
    "                base_e_reader = BaseEventReader(filename=e_path, eliminate_events_with_no_eeg=True)\n",
    "                base_events = base_e_reader.read()\n",
    "                base_events = base_events[base_events.type == 'PROB']\n",
    "\n",
    "                # ------------------- READING TAL STRUCTS\n",
    "                tal_reader = TalReader(filename=tal_path)\n",
    "                monopolar_channels = tal_reader.get_monopolar_channels()\n",
    "                bipolar_pairs = tal_reader.get_bipolar_pairs()\n",
    "    # Changing to recarray\n",
    "    base_events = base_events.view(np.recarray)\n",
    "    \n",
    "    return base_events, tal_reader, bipolar_pairs, monopolar_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def read_subj_base_events_elec(study_name, subject):\n",
    "    # Read in subjects events and electrode positions\n",
    "    \n",
    "    if study_name == 'RAM_FR1_CatFR1':\n",
    "        \n",
    "        # ------------------- READING EVENTS\n",
    "        # RAM_FR1\n",
    "        e_path = '/data/events/RAM_FR1/'+subject+'_math.mat'\n",
    "        base_e_reader = BaseEventReader(filename=e_path, eliminate_events_with_no_eeg=True)\n",
    "        FR1_base_events = base_e_reader.read()\n",
    "        FR1_base_events = FR1_base_events[FR1_base_events.type == 'PROB']\n",
    "        \n",
    "        # RAM_CatFR1\n",
    "        e_path = '/data/events/RAM_CatFR1/'+subject+'_math.mat'\n",
    "        base_e_reader = BaseEventReader(filename=e_path, eliminate_events_with_no_eeg=True)\n",
    "        CatFR1_base_events = base_e_reader.read()\n",
    "        CatFR1_base_events = CatFR1_base_events[CatFR1_base_events.type == 'PROB']\n",
    "        \n",
    "        # Making the CatFR1 sessions have unique numbering (added as additional session to FR1)\n",
    "        CatFR1_base_events.session = CatFR1_base_events.session + max(np.unique(FR1_base_events.session))+1\n",
    "\n",
    "        # Combining the base_events where subjects completed both FR1 and CatFR1\n",
    "        base_events = np.hstack((FR1_base_events,CatFR1_base_events))\n",
    "        \n",
    "        # ------------------- READING TAL STRUCTS\n",
    "        tal_path = '/data/eeg/'+subject+'/tal/'+subject+'_talLocs_database_bipol.mat'\n",
    "        tal_reader = TalReader(filename=tal_path)\n",
    "        monopolar_channels = tal_reader.get_monopolar_channels()\n",
    "        bipolar_pairs = tal_reader.get_bipolar_pairs()\n",
    "           \n",
    "    else:\n",
    "        # Regular pathway\n",
    "        e_path = '/data/events/'+study_name+'/'+subject+'_math.mat'\n",
    "        tal_path = '/data/eeg/'+subject+'/tal/'+subject+'_talLocs_database_bipol.mat'\n",
    "\n",
    "        # ------------------- READING EVENTS\n",
    "        base_e_reader = BaseEventReader(filename=e_path, eliminate_events_with_no_eeg=True)\n",
    "        base_events = base_e_reader.read()\n",
    "        base_events = base_events[base_events.type == 'PROB']\n",
    "\n",
    "        # ------------------- READING TAL STRUCTS\n",
    "        tal_reader = TalReader(filename=tal_path)\n",
    "        monopolar_channels = tal_reader.get_monopolar_channels()\n",
    "        bipolar_pairs = tal_reader.get_bipolar_pairs()\n",
    "        \n",
    "    # Changing to recarray\n",
    "    base_events = base_events.view(np.recarray)\n",
    "    \n",
    "    return base_events, tal_reader, bipolar_pairs, monopolar_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_eeg_data(base_events,monopolar_channels):\n",
    "    # Reading in events with the response time as the end time in seconds\n",
    "    data = [] # intialize the list where to store each event object\n",
    "\n",
    "    for i in range(0,base_events.shape[0]):\n",
    "\n",
    "        eeg_reader = EEGReader(events=base_events[i:i+1], channels=monopolar_channels,\n",
    "                               start_time=0.0, end_time=float(base_events[i]['rectime'])/1000, buffer_time=1.0)\n",
    "\n",
    "        base_eegs = eeg_reader.read() # temporary reading of events\n",
    "        data.append(base_eegs) # list of event objects \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def wavelet_function(data, freqs,bipolar_pairs):\n",
    "    # Specify event or range of events\n",
    "    data_pow_wavelet = []\n",
    "    #freqs=np.logspace(np.log10(3), np.log10(180), 8)\n",
    "    \n",
    "    for i_event in range(0,len(data)):\n",
    "\n",
    "        # Creating bipolar EEGs \n",
    "        m2b = MonopolarToBipolarMapper(time_series=data[i_event], bipolar_pairs=bipolar_pairs)\n",
    "        bp_eegs = m2b.filter()\n",
    "\n",
    "        # Filtering out line noise\n",
    "        b_filter = ButterworthFilter(time_series=bp_eegs, freq_range=[58., 62.], filt_type='stop', order=4)\n",
    "        bp_eegs_filtered = b_filter.filter()\n",
    "\n",
    "        # Perform the Morlet Wavelet\n",
    "        wf = MorletWaveletFilter(time_series=bp_eegs_filtered,\n",
    "                             freqs=freqs,\n",
    "                             output='power'                   \n",
    "                             )\n",
    "        pow_wavelet, phase_wavelet = wf.filter()\n",
    "\n",
    "        # Remove the buffer time period\n",
    "        pow_wavelet = pow_wavelet.remove_buffer(duration=1.0)\n",
    "        np.log10(pow_wavelet.data, out=pow_wavelet.data); # due to power decay in high freq\n",
    "\n",
    "        # Re-organize data structure and get rid of event dimension, since it is always 1\n",
    "        pow_wavelet = pow_wavelet.transpose(\"events\",\"bipolar_pairs\",\"frequency\",\"time\") # Transpose for subsequent analysis\n",
    "        pow_wavelet = np.squeeze(pow_wavelet) # Eliminating event dimension\n",
    "        \n",
    "        # Adding the data to a wavelet power structure\n",
    "        data_pow_wavelet.append(pow_wavelet)\n",
    "    return data_pow_wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def wavelet_downsample(data_pow_wavelet,downsampled_freq):\n",
    "    # Down-sampling after the wavelet transform\n",
    "#     downsampled_freq = 100 # If need to hard-code downsampled frequency\n",
    "\n",
    "    data_pow_wavelet_ds = []\n",
    "    # Loops through all events and downsamples to input frequency\n",
    "    for i_event in range(0,len(data_pow_wavelet)):\n",
    "        \n",
    "        # Old version of downsampling\n",
    "        # data_pow_wavelet_temp_ds = data_pow_wavelet[i_event].resampled(downsampled_freq) # Downsampling each event \n",
    "        # Updated version of downsamping\n",
    "        resample_filter = ResampleFilter(time_series=data_pow_wavelet[i_event], resamplerate=100.0)\n",
    "        data_pow_wavelet_temp_ds = resample_filter.filter()\n",
    "        \n",
    "        # IMPORTANT: Adds event data to attributes\n",
    "        data_pow_wavelet_temp_ds.attrs['events'] = data_pow_wavelet[i_event].events\n",
    "        \n",
    "        data_pow_wavelet_temp_ds.coords['samplerate'] = downsampled_freq # Adjusting samplerate to downsampled frequency\n",
    "        data_pow_wavelet_ds.append(data_pow_wavelet_temp_ds)      \n",
    "    return data_pow_wavelet_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def z_score_function(data_pow_wavelet):\n",
    "    ## Calculating the z score\n",
    "    mean_powers_by_event = []\n",
    "    downsampled_freq = 100\n",
    "\n",
    "    # Averaging across the time dimension\n",
    "    for j_events in range(0,len(data_pow_wavelet)):\n",
    "        temp_mean_powers = np.array(data_pow_wavelet[j_events].mean(skipna=True,axis=2)) # Averages across time dimension (last dimension)\n",
    "        mean_powers_by_event.append(temp_mean_powers) # (events, electrode pairs, frequency bands)\n",
    "\n",
    "    # Mean across events\n",
    "    mean_powers = np.nanmean(mean_powers_by_event,axis=0) # mean power across events, now have (electrode pairs, frequency)\n",
    "    std_powers = np.nanstd(mean_powers_by_event,axis=0) # Standard deviations of the means of each event (electrode pairs, frequency)\n",
    "\n",
    "    # Re-creating the matrix with time and events\n",
    "    data_mean_pow = []\n",
    "    data_std_pow = []\n",
    "    for k_events in range(0,len(data_pow_wavelet)):\n",
    "        mean_pow_w_time = np.tile(mean_powers,[len(data_pow_wavelet[k_events].time),1,1]) # tiles along time dimension\n",
    "        mean_pow_w_time = np.transpose(mean_pow_w_time,(1,2,0)) # re-arrange to dimensions in pow_wavelet (electrode pairs, frequency, time)\n",
    "        data_mean_pow.append(mean_pow_w_time) # Adds tiled matrices to events structure\n",
    "\n",
    "        std_pow_w_time = np.tile(std_powers,[len(data_pow_wavelet[k_events].time),1,1]) # tiles along time dimension for std\n",
    "        std_pow_w_time = np.transpose(std_pow_w_time,(1,2,0)) # re-arrange to dimensions in pow_wavelet (electrode pairs, frequency, time)\n",
    "        data_std_pow.append(std_pow_w_time) # Adds tiled matrices to events structure\n",
    "\n",
    "    # Z score calculation\n",
    "    data_z_pow_wavelet = [] # Comment out if you want to add more events (more than one session)\n",
    "\n",
    "    for i_event in range(0,len(data_pow_wavelet)):\n",
    "        z_pow_wavelet = (data_pow_wavelet[i_event] - data_mean_pow[i_event])/data_std_pow[i_event] # Z score equation\n",
    "#         z_pow_wavelet = z_pow_wavelet.resampled(downsampled_freq) # Downsampling each event \n",
    "        data_z_pow_wavelet.append(z_pow_wavelet) # Blocks of (electrode pairs, frequency, time) for each event\n",
    "    \n",
    "    return data_z_pow_wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def z_score_function_updated(data_pow_wavelet):\n",
    "    import copy\n",
    "    ## Calculating the z score\n",
    "    mean_powers_by_event = []\n",
    "    data_z_pow_wavelet = [] # Comment out if you want to add more events (more than one session)\n",
    "\n",
    "    downsampled_freq = 100\n",
    "\n",
    "    # Concatenates all events together to set up calculation of mean and standard deviation below\n",
    "    for j_events in range(0,len(data_pow_wavelet)):\n",
    "        if j_events == 0:\n",
    "            data_complt_session = data_pow_wavelet[0]\n",
    "        else:\n",
    "            data_complt_session = np.concatenate((data_complt_session,data_pow_wavelet[j_events]),axis=2) \n",
    "        \n",
    "    # Mean across time\n",
    "    mean_powers = np.nanmean(data_complt_session,axis=2) # mean power across events, now have (electrode pairs, frequency)\n",
    "    std_powers = np.nanstd(data_complt_session,axis=2) # Standard deviations of the means of each event (electrode pairs, frequency)\n",
    "\n",
    "    # Re-creating the matrix with time and events\n",
    "    data_mean_pow = []\n",
    "    data_std_pow = []\n",
    "    for k_events in range(0,len(data_pow_wavelet)):\n",
    "        mean_pow_w_time = np.tile(mean_powers,[len(data_pow_wavelet[k_events].time),1,1]) # tiles along time dimension\n",
    "        mean_pow_w_time = np.transpose(mean_pow_w_time,(1,2,0)) # re-arrange to dimensions in pow_wavelet (electrode pairs, frequency, time)\n",
    "        data_mean_pow.append(mean_pow_w_time) # Adds tiled matrices to events structure\n",
    "\n",
    "        std_pow_w_time = np.tile(std_powers,[len(data_pow_wavelet[k_events].time),1,1]) # tiles along time dimension for std\n",
    "        std_pow_w_time = np.transpose(std_pow_w_time,(1,2,0)) # re-arrange to dimensions in pow_wavelet (electrode pairs, frequency, time)\n",
    "        data_std_pow.append(std_pow_w_time) # Adds tiled matrices to events structure\n",
    "\n",
    "    # Z score calculation\n",
    "\n",
    "    for i_event in range(0,len(data_pow_wavelet)):\n",
    "        z_pow_wavelet = (data_pow_wavelet[i_event] - data_mean_pow[i_event])/data_std_pow[i_event] # Z score equation\n",
    "        z_pow_wavelet.attrs = data_pow_wavelet[i_event].attrs # Keep attributes\n",
    "        data_z_pow_wavelet.append(copy.deepcopy(z_pow_wavelet)) # Blocks of (electrode pairs, frequency, time) for each event\n",
    "    \n",
    "    return data_z_pow_wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_z_pow_wavelet(base_events,freqs,downsampled_freq,bipolar_pairs,monopolar_channels):\n",
    "    # Loading EEG data, calculating Wavelet power, and performing z-score across multiple sessions\n",
    "    #freqs=np.logspace(np.log10(3), np.log10(180), 8)\n",
    "    # downsampled_freq= 100.0\n",
    "\n",
    "    data_z_pow_wavelet = []\n",
    "\n",
    "    # Finding unique sessions while preserving order\n",
    "    uniq_indices = np.unique(base_events.session,return_index=True)[1]\n",
    "    uniq_sessions = [base_events.session[index] for index in sorted(uniq_indices)]\n",
    "    \n",
    "#     for i_sessions in range(min(base_events['session']),max(base_events['session'])+1): # Analyzing all of the subject's sessions - OLD WAY\n",
    "    for i_sessions in uniq_sessions: # Analyzing all of the subject's sessions\n",
    "        base_events_filt = base_events[base_events['session'] == i_sessions] # Filtering for each subject session\n",
    "\n",
    "        if len(base_events_filt) != 0:\n",
    "            data_temp = load_eeg_data(base_events_filt,monopolar_channels) # Loading the EEG data for the session\n",
    "\n",
    "            data_wavelet_temp = wavelet_function(data_temp,freqs,bipolar_pairs) # Performing the wavelet\n",
    "\n",
    "            data_wavelet_temp_ds = wavelet_downsample(data_wavelet_temp,downsampled_freq) # Downsamples\n",
    "\n",
    "            data_z_temp = z_score_function_updated(data_wavelet_temp_ds) # Z scores \n",
    "\n",
    "            data_z_pow_wavelet = data_z_pow_wavelet+data_z_temp\n",
    "    return data_z_pow_wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_ROI_elec(tal_reader,ROI,cortex_side,study_name):\n",
    "# Function finds all electrode in desired ROI and returns indices for analysis\n",
    "    elec_index = []\n",
    "        \n",
    "    # Superior Frontal Gyrus\n",
    "    if ROI == 'SFG':\n",
    "        ROI_regions = ['superiorfrontal']\n",
    "        \n",
    "    # Middle Frontal Gyrus\n",
    "    elif ROI == 'MFG':\n",
    "        \n",
    "        ROI_regions = ['caudalmiddlefrontal','rostralmiddlefrontal'] # regions defined by middle frontal gyrus\n",
    "        \n",
    "    # Inferior Frontal Gyrus\n",
    "    elif ROI == 'IFG':\n",
    "        ROI_regions = ['parsopercularis','parsorbitalis','parstriangularis'] # Regions defined by inferior frontal gyrus\n",
    "    \n",
    "    # Frontal Crotex (FC) # Added 2/2017\n",
    "    elif ROI == 'FC':\n",
    "        \n",
    "        ROI_regions = ['superiorfrontal','caudalmiddlefrontal','rostralmiddlefrontal','parsopercularis','parsorbitalis','parstriangularis'] # Includes inferior, middle, superior regions\n",
    "    \n",
    "    # Occipital Cortex\n",
    "    elif ROI == 'OC':\n",
    "        ROI_regions = ['lateraloccipital','lingual','cuneus','pericalcarine']\n",
    "    \n",
    "    # Temporal Cortex\n",
    "    elif ROI == 'TC':\n",
    "        ROI_regions = ['superiortemporal','middletemporal','inferiortemporal'] # Previously included \"Brodman area 37\" but was not recognizing subjects, superiortemporal added 12/2017\n",
    "        \n",
    "    # Inferior Parietal Cortex\n",
    "    elif ROI == 'IPC':\n",
    "        ROI_regions = ['inferiorparietal','supramarginal']\n",
    "        \n",
    "    # Superior Parietal Cortex\n",
    "    elif ROI == 'SPC':\n",
    "        ROI_regions = ['superiorparietal','precuneus']\n",
    "        \n",
    "    # Parietal Cortex (PC) # Added 2/2017\n",
    "    elif ROI == 'PC':\n",
    "        ROI_regions = ['inferiorparietal','supramarginal','superiorparietal','precuneus']\n",
    "        \n",
    "    # Fusiform Gyrus\n",
    "    elif ROI == 'FUS':\n",
    "        ROI_regions = ['fusiform']\n",
    "        \n",
    "    ## Areas within parietal cortex separated # Added 12/2017\n",
    "    elif ROI == 'IPG':\n",
    "        ROI_regions = ['inferiorparietal']\n",
    "    elif ROI == 'SPG':\n",
    "        ROI_regions = ['superiorparietal']\n",
    "    elif ROI == 'SMG':\n",
    "        ROI_regions = ['supramarginal']\n",
    "    elif ROI == 'PRC':\n",
    "        ROI_regions = ['precuneus']\n",
    "        \n",
    "    ## Areas within the temporal cortex separated # Added 12/2017\n",
    "    elif ROI == 'ITG':\n",
    "        ROI_regions = ['inferiortemporal']     \n",
    "    elif ROI == 'MTG':\n",
    "        ROI_regions = ['middletemporal']\n",
    "    elif ROI == 'STG':\n",
    "        ROI_regions = ['superiortemporal']\n",
    "    \n",
    "    # Motor Cortex - Pre-Central Gyrus\n",
    "    elif ROI == 'MC':\n",
    "        ROI_regions = ['precentral']\n",
    "        \n",
    "    # Hippocampus (only for RAM subjects -- locTags) Subjects in pyFR have different function\n",
    "    elif ROI == 'HIPP':\n",
    "        ROI_regions = ['Left CA1','Left CA2','Left CA3','Left DG','Left Sub','Right CA1','Right CA2','Right CA3','Right DG','Right Sub']\n",
    "        \n",
    "    # Medial Temporal Lobe (only for RAM subjects -- locTags)\n",
    "    elif ROI == 'MTL':\n",
    "        ROI_regions = ['Left PRC','Right PRC','Left EC','Right EC','Left PHC','Right PHC']\n",
    "        \n",
    "        if ('locTag' in tal_reader.tal_struct_array.dtype.names == 'False') or (study_name == 'pyFR'): # If the location tags are not defined, uses Freesurfer segmentation\n",
    "            ROI_regions = ['parahippocampal','entorhinal']\n",
    "            ROI = 'MTL_noLocs' # Re-naming so code will not enter locTags if statement\n",
    "    \n",
    "    ## Locating all electrodes within the ROi regions defined above\n",
    "    if (ROI == 'HIPP') and (study_name != 'pyFR') or (ROI == 'MTL') and (study_name != 'pyFR'):\n",
    "        # These two ROIs are defined by the locTag structure\n",
    "        for i_regions in ROI_regions:\n",
    "            # Returns the indices of electrodes within the ROI\n",
    "            elec_index_temp = np.where(tal_reader.tal_struct_array.locTag == i_regions)[0]\n",
    "\n",
    "            # Adds current region's electrodes to previously identifed electrodes\n",
    "            elec_index = np.hstack((elec_index,elec_index_temp))\n",
    "    elif (ROI == 'HIPP') and (study_name == 'pyFR'):\n",
    "        # pyFR study uses a different localization\n",
    "        if study_name == 'pyFR':\n",
    "            bipolar_pairs = tal_reader.get_bipolar_pairs()\n",
    "            subject = tal_reader.tal_struct_array['subject'][0]\n",
    "            elec_index = find_hip_elec_pyFR(subject,bipolar_pairs,tal_reader)\n",
    "    else:\n",
    "        # All regions other than the ones above use the avgSurf field for localization\n",
    "        for i_regions in ROI_regions:\n",
    "            # Returns the indices of electrodes within the ROI\n",
    "            elec_index_temp = np.where(tal_reader.tal_struct_array.avgSurf['anatRegion'] == i_regions)[0]\n",
    "\n",
    "            # Adds current region's electrodes to previously identifed electrodes\n",
    "            elec_index = np.hstack((elec_index,elec_index_temp))\n",
    "    \n",
    "    elec_index = elec_index.astype('int') # Formatted to index arrays\n",
    "    \n",
    "    # Localizing electrodes to a particular side (i.e. Left or Right or Both)\n",
    "    if cortex_side == 'Left':\n",
    "        side_index = np.where(tal_reader.tal_struct_array.x < 0)[0] # Negative values in the x dimension indicate left cortex\n",
    "        elec_index = np.intersect1d(side_index,elec_index) # Finding the overlap between the ROI and the cortex side\n",
    "    elif cortex_side == 'Right':\n",
    "        side_index = np.where(tal_reader.tal_struct_array.x > 0)[0] # Positive values in the x dimension indicate right cortex\n",
    "        elec_index = np.intersect1d(side_index,elec_index) # Finding the overlap between the ROI and the cortex side\n",
    "    \n",
    "    return elec_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_hip_elec_pyFR(subject,bipolar_pairs,tal_reader):\n",
    "# Locating hippocampal electrodes for pyFR\n",
    "    \n",
    "    # Loading Ethan's hippocampal electrode file\n",
    "    import scipy.io\n",
    "    mat_file = scipy.io.loadmat('/data6/scratch/esolo/pyFR_hippocampal_elecs.mat') # Loading array with electrodes within hippocampus\n",
    "    hip_electrodes = squeeze(mat_file['he']) # Extracting the hippocampal electrode variable from the .mat file\n",
    "\n",
    "    # Redefining the bipolar_pairs as an integer array in order to compare with hipp array\n",
    "    bipolar_pairs_int = np.zeros([len(bipolar_pairs),2])\n",
    "    for i_pairs in range(0,len(bipolar_pairs)):\n",
    "        [a,b] = bipolar_pairs[i_pairs]\n",
    "        elec_pair_temp = [int(a),int(b)]\n",
    "        bipolar_pairs_int[i_pairs,:] = elec_pair_temp\n",
    "\n",
    "    # Selecting hippocampal electrodes\n",
    "    hip_subj_elec = hip_electrodes[hip_electrodes['subject'] == subject]\n",
    "\n",
    "    # Finding the indices of hippocampal electrodes within the bipolar_pairs\n",
    "    elec_index = np.zeros(len(hip_subj_elec))\n",
    "    num_hip_pairs = 0\n",
    "    for i_hipp_pair in hip_subj_elec['channel']:\n",
    "        for i_elec_pair in range(0,len(bipolar_pairs)):\n",
    "            if np.array_equal(bipolar_pairs_int[i_elec_pair,:],i_hipp_pair[0]):\n",
    "                elec_index[num_hip_pairs] = i_elec_pair\n",
    "                num_hip_pairs = num_hip_pairs+1\n",
    "    \n",
    "    elec_index = elec_index.astype(int) # convert to integers\n",
    "    \n",
    "    ## MADE OBSOLETE - KEPT FOR REFERENCE\n",
    "#     # Filters the electrodes based on hemisphere \n",
    "#     if len(elec_index) != 0: \n",
    "#         elec_boolean = []\n",
    "#         if cortex_side == 'Right':\n",
    "#             for i_elec in elec_index:\n",
    "#                 elec_boolean = elec_boolean + ([tal_reader.tal_struct_array[['Loc1']][i_elec][0] == 'Right Cerebrum'])\n",
    "#             elec_index = elec_index[array(elec_boolean)]\n",
    "#         elif cortex_side == 'Left':\n",
    "#             for i_elec in elec_index:\n",
    "#                 elec_boolean = elec_boolean + ([tal_reader.tal_struct_array[['Loc1']][i_elec][0] == 'Left Cerebrum'])\n",
    "#             elec_index = elec_index[array(elec_boolean)]\n",
    "    \n",
    "    # print elec_index\n",
    "    return elec_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_correct(data_z_pow_wavelet,base_events):\n",
    "    # Creating index for all correct events\n",
    "    correct_index = np.where(base_events['iscorrect'] == 1)[0]\n",
    "\n",
    "    # Creating an index for all incorrect events\n",
    "    incorrect_index = np.where(base_events['iscorrect'] == 0)[0]\n",
    "\n",
    "    # Only includes events that have a correct response\n",
    "    data_z_pow_wavelet_cor = [data_z_pow_wavelet[i] for i in correct_index]\n",
    "\n",
    "    # Only includes events that have an incorrect response\n",
    "    data_z_pow_wavelet_incor = [data_z_pow_wavelet[i] for i in incorrect_index]\n",
    "    \n",
    "    # Only including correct events in base_events\n",
    "    events_boolean_cor = base_events['iscorrect'] == 1\n",
    "    base_events = base_events[events_boolean_cor]\n",
    "    \n",
    "    return data_z_pow_wavelet_cor, data_z_pow_wavelet_incor, base_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def separate_rectime(data_z_pow_wavelet,base_events):\n",
    "# Creates arrays separated at the median by the response time\n",
    "\n",
    "    # Initialize all variables\n",
    "    rectimes_by_event = []\n",
    "    data_z_pow_wavelet_fst = [];\n",
    "    data_z_pow_wavelet_slw = [];\n",
    "\n",
    "    # Calculates mean response time\n",
    "#     rectimes_by_event_mean = base_events['rectime'] < mean(base_events['rectime']) # Alternate approach -- differing number of subjects\n",
    "    rectimes_by_event_mean = base_events['rectime'] < median(base_events['rectime']) # Alternate approach -- differing number of subjects\n",
    "    \n",
    "#     for i_event in range(0,len(data_z_pow_wavelet)):\n",
    "#         rectimes_by_event.append(data_z_pow_wavelet[i_event].attrs['events'].values['rectime'])\n",
    "#     mean_rectime = mean(rectimes_by_event) # Changed to mean\n",
    "#     rectimes_by_event_mean = rectimes_by_event < mean_rectime # 'True' means it is a fast event\n",
    "\n",
    "    # Separating fast and slow\n",
    "    for i_events in range(0,len(rectimes_by_event_mean)):\n",
    "        if rectimes_by_event_mean[i_events]:\n",
    "            data_z_pow_wavelet_fst.append(data_z_pow_wavelet[i_events]) # Finding all fast events\n",
    "        else:\n",
    "            data_z_pow_wavelet_slw.append(data_z_pow_wavelet[i_events]) # Finding all slow events\n",
    "    \n",
    "    return data_z_pow_wavelet_fst,data_z_pow_wavelet_slw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def separate_rectime_model(data_z_pow_wavelet,base_events):\n",
    "    ## Loading the model for trial separation\n",
    "    filename = 'ECoG_Model.sav'\n",
    "    num_features = 5\n",
    "    lin_regression = pickle.load(open(filename, 'rb'))\n",
    "    model_coef_total = lin_regression.coef_[0:num_features] # Rest of the features are subject-related\n",
    "\n",
    "    ## Model Factors/ Features\n",
    "    problem_set = base_events.test\n",
    "\n",
    "    # Extracting digits from problem\n",
    "    first_digit = array([x[0] for x in problem_set])\n",
    "    second_digit = array([y[1] for y in problem_set])\n",
    "    third_digit =  array([z[2] for z in problem_set])\n",
    "\n",
    "    # Problem Sum\n",
    "    problem_sum = array(map(sum,problem_set)) # Adds the digits in each tuple which are the (x,y,z) values\n",
    "\n",
    "    # Odd/Even Sum\n",
    "    odd_even_sum = (problem_sum % 2 == 0).astype(int) # If sum of problem equals zero when divided by two\n",
    "\n",
    "    # Triplet\n",
    "    triplet = (first_digit == second_digit) & (second_digit == third_digit).astype(int)\n",
    "\n",
    "    # Repeated Digits\n",
    "    repeated_doubles = (first_digit == second_digit) | (second_digit == third_digit) | (first_digit == third_digit).astype(int)\n",
    "    repeated_doubles = repeated_doubles - triplet # Remove counting triplets twice\n",
    "\n",
    "    # Two Digits adding to multiples of 10\n",
    "    multiple_of_ten = ((first_digit+second_digit) % 10 == 0) | ((second_digit+third_digit) % 10 == 0) | ((first_digit+third_digit) % 10 == 0).astype(int)\n",
    "\n",
    "    ## Concatenate all factors into array\n",
    "    RT_features = np.transpose(np.vstack((problem_sum,odd_even_sum,triplet,repeated_doubles,multiple_of_ten)))\n",
    "\n",
    "    ## Calculate predicted response times\n",
    "    rectime_predict = np.matmul(RT_features,model_coef_total)\n",
    "\n",
    "    ## Separate trial based on predicted rectime\n",
    "\n",
    "    # Initialize all variables\n",
    "    data_z_pow_wavelet_fst = [];\n",
    "    data_z_pow_wavelet_slw = [];\n",
    "\n",
    "    # Calculate residuals after model prediction\n",
    "    rectime_residual = base_events['rectime'] - rectime_predict\n",
    "    \n",
    "    # Separate by mean\n",
    "    rectimes_by_event_mean = rectime_residual < median(rectime_residual) # Alternate approach -- differing number of subjects\n",
    "\n",
    "    # Separate into slow and fast datasets\n",
    "    for i_events in range(0,len(rectimes_by_event_mean)):\n",
    "        if rectimes_by_event_mean[i_events]:\n",
    "            data_z_pow_wavelet_fst.append(data_z_pow_wavelet[i_events]) # Finding all fast events\n",
    "        else:\n",
    "            data_z_pow_wavelet_slw.append(data_z_pow_wavelet[i_events]) # Finding all slow events\n",
    "\n",
    "    return data_z_pow_wavelet_fst,data_z_pow_wavelet_slw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def separate_rectime_predict(data_z_pow_wavelet,base_events):\n",
    "    ## Loading the model for trial separation\n",
    "    filename = 'ECoG_Model.sav'\n",
    "    num_features = 5\n",
    "    lin_regression = pickle.load(open(filename, 'rb'))\n",
    "    model_coef_total = lin_regression.coef_[0:num_features] # Rest of the features are subject-related\n",
    "\n",
    "    ## Model Factors/ Features\n",
    "    problem_set = base_events.test\n",
    "\n",
    "    # Extracting digits from problem\n",
    "    first_digit = array([x[0] for x in problem_set])\n",
    "    second_digit = array([y[1] for y in problem_set])\n",
    "    third_digit =  array([z[2] for z in problem_set])\n",
    "\n",
    "    # Problem Sum\n",
    "    problem_sum = array(map(sum,problem_set)) # Adds the digits in each tuple which are the (x,y,z) values\n",
    "\n",
    "    # Odd/Even Sum\n",
    "    odd_even_sum = (problem_sum % 2 == 0).astype(int) # If sum of problem equals zero when divided by two\n",
    "\n",
    "    # Triplet\n",
    "    triplet = (first_digit == second_digit) & (second_digit == third_digit).astype(int)\n",
    "\n",
    "    # Repeated Digits\n",
    "    repeated_doubles = (first_digit == second_digit) | (second_digit == third_digit) | (first_digit == third_digit).astype(int)\n",
    "    repeated_doubles = repeated_doubles - triplet # Remove counting triplets twice\n",
    "\n",
    "    # Two Digits adding to multiples of 10\n",
    "    multiple_of_ten = ((first_digit+second_digit) % 10 == 0) | ((second_digit+third_digit) % 10 == 0) | ((first_digit+third_digit) % 10 == 0).astype(int)\n",
    "\n",
    "    ## Concatenate all factors into array\n",
    "    RT_features = np.transpose(np.vstack((problem_sum,odd_even_sum,triplet,repeated_doubles,multiple_of_ten)))\n",
    "\n",
    "    ## Calculate predicted response times\n",
    "    rectime_predict = np.matmul(RT_features,model_coef_total)\n",
    "\n",
    "    ## Separate trial based on predicted rectime\n",
    "\n",
    "    # Initialize all variables\n",
    "    data_z_pow_wavelet_fst = [];\n",
    "    data_z_pow_wavelet_slw = [];\n",
    "    \n",
    "    # Separate by mean\n",
    "    rectimes_by_event_mean = rectime_predict < median(rectime_predict) # Alternate approach -- differing number of subjects\n",
    "\n",
    "    # Separate into slow and fast datasets\n",
    "    for i_events in range(0,len(rectimes_by_event_mean)):\n",
    "        if rectimes_by_event_mean[i_events]:\n",
    "            data_z_pow_wavelet_fst.append(data_z_pow_wavelet[i_events]) # Finding all fast events\n",
    "        else:\n",
    "            data_z_pow_wavelet_slw.append(data_z_pow_wavelet[i_events]) # Finding all slow events\n",
    "\n",
    "    return data_z_pow_wavelet_fst,data_z_pow_wavelet_slw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def avg_time_bins(data_z_pow_wavelet,elec_index,time_bins):\n",
    "    # Averaging across time bins for each event\n",
    "    # time_bins = 10\n",
    "    samp_freq = np.array(data_z_pow_wavelet[0].samplerate) # given in Hz (should also work for downsampled data)\n",
    "#     samp_freq = 100 # Switch if using downsampled data is used\n",
    "    \n",
    "    # Initializing array size\n",
    "    time_bin_power = np.zeros((len(data_z_pow_wavelet),len(data_z_pow_wavelet[0].bipolar_pairs),len(data_z_pow_wavelet[0].frequency),time_bins))\n",
    "    time_bin_power_std = np.zeros((len(data_z_pow_wavelet),len(data_z_pow_wavelet[0].bipolar_pairs),len(data_z_pow_wavelet[0].frequency),time_bins))\n",
    "\n",
    "    for k_event in range(0,len(data_z_pow_wavelet)):   \n",
    "        bin_interval = float(np.array(data_z_pow_wavelet[k_event].time[-1])/time_bins) # Setting the bin interval for each event\n",
    "        for i_bins in range(0,time_bins):\n",
    "            # Calculating the bin indexing for each iteration\n",
    "            lower_bin = int(round(bin_interval*float(i_bins)*samp_freq))\n",
    "            upper_bin = int(round(bin_interval*float(i_bins+1)*samp_freq))\n",
    "            time_bin_power[k_event,:,:,i_bins] = nanmean(np.array(data_z_pow_wavelet[k_event][:,:,lower_bin:upper_bin]),axis=2)\n",
    "            # Standardizes each event with time bins Structure: (events, electrode_pairs, frequency,time bins)\n",
    "    \n",
    "    # Average across selected electrodes \n",
    "    time_bin_power_avg_elec = time_bin_power[:,elec_index,:,:] # Reducing to only selected electrodes\n",
    "    \n",
    "    # Averages across electrodes if more than one electrode in ROI\n",
    "    if len(elec_index) > 1:\n",
    "        time_bin_power_avg_elec = nanmean(time_bin_power_avg_elec,axis=1) # Average across selected electrodes\n",
    "    else: # Reducing the extra dimension if one electrode\n",
    "        time_bin_power_avg_elec = np.squeeze(time_bin_power_avg_elec,axis=1)\n",
    "        \n",
    "        \n",
    "    # Averages across events and calculates the standard deviation\n",
    "    time_bin_power_avg = nanmean(time_bin_power_avg_elec,axis=0) # Average across events\n",
    "#     time_bin_power_std_avg = std(time_bin_power_avg_elec,axis=0) # Standard deviation across events (not necessary since standard deviation will be calculated later)  \n",
    "        \n",
    "    return time_bin_power_avg, time_bin_power_avg_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_tstat(timebin_pow_events_fst,timebin_pow_events_slw,time_bins):\n",
    "# Calculates the t-statistic for each frequency band and time bin\n",
    "    from scipy import stats # import for t test\n",
    "\n",
    "    num_freqs = np.shape(timebin_pow_events_fst)[1]\n",
    "    \n",
    "    tscore_results = np.zeros([num_freqs,time_bins]) # Length of the array is freqs x timebins\n",
    "\n",
    "    # Calculating the t statistic between fast and slow events for each subject\n",
    "    for i_freq in range(0,num_freqs):\n",
    "        for i_timebins in range(0,time_bins):\n",
    "            tresults_temp = stats.ttest_ind(array(timebin_pow_events_fst)[:,i_freq,i_timebins],array(timebin_pow_events_slw)[:,i_freq,i_timebins])\n",
    "            tscore_results[i_freq,i_timebins] = tresults_temp.statistic # Fills an array of t_statistic for each subject\n",
    "                \n",
    "    return tscore_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_tstat_ROI(data_z_pow_wavelet_fst,data_z_pow_wavelet_slw,freqs):\n",
    "    # Calculates t-statistic for every frequency for each subject and ROI\n",
    "    \n",
    "    from scipy import stats # import for t test\n",
    "    \n",
    "    # Initializing variables\n",
    "    ttest_ROI = np.empty((len(freqs)))\n",
    "    data_zpow_fst = np.empty((len(data_z_pow_wavelet_fst),len(data_z_pow_wavelet_fst[0].bipolar_pairs),len(data_z_pow_wavelet_fst[0].frequency)),dtype=float) # Array [Events, bipolar_pairs, frequency]\n",
    "    data_zpow_slw = np.empty((len(data_z_pow_wavelet_slw),len(data_z_pow_wavelet_slw[0].bipolar_pairs),len(data_z_pow_wavelet_slw[0].frequency)),dtype=float)\n",
    "\n",
    "    # Averaging over time dimesnion and re-structuring to array with events as a dimension\n",
    "    for i_events in range(0,len(data_z_pow_wavelet_fst)):\n",
    "        data_zpow_fst[i_events,:,:] = data_z_pow_wavelet_fst[i_events].mean(skipna=True,axis=-1) # [Bipolar_pairs, frequency] adding to an array with events dimension\n",
    "    for i_events in range(0,len(data_z_pow_wavelet_slw)):\n",
    "        data_zpow_slw[i_events,:,:] = data_z_pow_wavelet_slw[i_events].mean(skipna=True,axis=-1) # [Bipolar_pairs, frequency] adding to an array with events dimension\n",
    "    data_mean_zpow_fst = nanmean(data_zpow_fst[:,elec_index,:],axis=1) # Averaging ONLY bipolar pairs within the ROI for fast events\n",
    "    data_mean_zpow_slw = nanmean(data_zpow_slw[:,elec_index,:],axis=1) # Above with slow events\n",
    "\n",
    "    # ttest between events for each frequency band\n",
    "    for i_freqs in range(0,len(data_z_pow_wavelet_fst[0].frequency)):\n",
    "        ttest_result = stats.ttest_ind(data_mean_zpow_fst[:,i_freqs],data_mean_zpow_slw[:,i_freqs])\n",
    "        ttest_ROI[i_freqs] = ttest_result.statistic    \n",
    "        \n",
    "    return ttest_ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_tstat_elec(data_z_pow_wavelet_fst, data_z_pow_wavelet_slw,low_freq,high_freq):\n",
    "############# UNFINISHED ##########\n",
    "# Calculates the t-statistic for each electrode pair for cortical plots\n",
    "\n",
    "    from scipy import stats # import for t test\n",
    "\n",
    "    num_freqs = np.shape(timebin_pow_events_fst)[1]\n",
    "    \n",
    "    # Calculating average power\n",
    "    \n",
    "    \n",
    "    tscore_results = np.zeros([num_freqs,time_bins]) # Length of the array is freqs x timebins\n",
    "\n",
    "    # Calculating the t statistic between fast and slow events for each subject\n",
    "    for i_freq in range(0,num_freqs):\n",
    "        for i_timebins in range(0,time_bins):\n",
    "            tresults_temp = stats.ttest_ind(array(timebin_pow_events_fst)[:,i_freq,i_timebins],array(timebin_pow_events_slw)[:,i_freq,i_timebins])\n",
    "            tscore_results[i_freq,i_timebins] = tresults_temp.statistic # Fills an array of t_statistic for each subject\n",
    "                \n",
    "    return tscore_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_time_bin_avg(time_bin_power_avg,time_bin_power_std_avg,time_bins,z_range,main_title,file_path):\n",
    "    # Plot average time-bin events for each frequency\n",
    "    \n",
    "    import matplotlib.cm as cm\n",
    "    from pylab import locator_params\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(freqs)))\n",
    "\n",
    "    time_bin_label = range(1,time_bins+1)\n",
    "    frequency_labels = ('3 Hz', '5 Hz', '10 Hz','17 Hz','31 Hz','55 Hz','100 Hz','180 Hz')\n",
    "\n",
    "    fig, axs = plt.subplots(3, 3, sharey = True,figsize=(6,4)) # Plots only 8 frequncy bands\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(top=0.88,bottom=.1,left=0.1,wspace=0.4,hspace=0.5)\n",
    "    y_data = np.transpose(time_bin_power_avg[:,:]) # Simplify plot command below # (electrode pairs, frequency, time bins)\n",
    "    y_std = np.transpose(time_bin_power_std_avg[:,:])\n",
    "\n",
    "    for i_plot in range(0,len(freqs)):\n",
    "        plt.subplot(3,3,i_plot+1)\n",
    "        plt.plot(time_bin_label,y_data[:,i_plot],color=colors[i_plot,:], label=frequency_labels[i_plot])\n",
    "        plt.fill_between(time_bin_label,y_data[:,i_plot]-y_std[:,i_plot],y_data[:,i_plot]+y_std[:,i_plot],alpha=.3,facecolor=colors[i_plot,:])\n",
    "        plt.axhline(0,color='black',linestyle='--') # Plot zero line\n",
    "        title(frequency_labels[i_plot],fontsize=10)\n",
    "        axis([1,time_bins, -(z_range), z_range])\n",
    "        locator_params(axis='y',nbins=4) # Sets the number of tick marks on the y axis\n",
    "\n",
    "        # Combination subplot\n",
    "        plt.subplot(3,3,9)\n",
    "        plt.plot(time_bin_label,y_data[:,i_plot],color=colors[i_plot,:], label=frequency_labels[i_plot])\n",
    "        plt.fill_between(time_bin_label,y_data[:,i_plot]-y_std[:,i_plot],y_data[:,i_plot]+y_std[:,i_plot],alpha=.3,facecolor=colors[i_plot,:])\n",
    "        plt.axhline(0,color='black',linestyle='--') # Plot zero line\n",
    "        title('All Frequencies',fontsize=10)\n",
    "        axis([1,time_bins, -(z_range),z_range])\n",
    "        locator_params(axis='y',nbins=4)\n",
    "        \n",
    "    # plt.legend(loc=4,prop={'size':9}) # For prior plotting method\n",
    "\n",
    "    # Entire figure labels\n",
    "    fig.text(0.5, 0.0, 'Time Bins', ha='center',size=12)\n",
    "    fig.text(0.0, 0.5, 'z Score', va='center', rotation='vertical',size=12)\n",
    "    fig.suptitle(main_title, fontsize=16)\n",
    "\n",
    "#     plt.savefig(file_path) # UNCOMMENT IF WANT IMAGE SAVED\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "#     savefig('time_bin_analysis.pdf') # Save figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def timefreq_average(data_z_pow_wavelet,base_events,elec_index,time_length,event_type,data_locked):\n",
    "# Averaging over events for a subset of electrdoes for either stimulus-locked or response-locked\n",
    "    # time_length = 4 # given in seconds\n",
    "    # data_locked: stimulus-locked = STIM, response-locked = RESP\n",
    "    # event_type = FAST or SLOW\n",
    "    \n",
    "    samp_freq = int(array(data_z_pow_wavelet[0].samplerate))\n",
    "    \n",
    "    # Only including correct events - COMMENTED because correct events should already be fitlered\n",
    "#     events_boolean_cor = base_events['iscorrect'] == 1\n",
    "#     base_events = base_events[events_boolean_cor]\n",
    "    \n",
    "    # Fast or Slow Events Separation\n",
    "    if event_type == 'FAST':\n",
    "        # Finding all of the fast events\n",
    "        events_boolean = base_events['rectime'] < median(base_events['rectime']) # Changed to median from mean\n",
    "    elif event_type == 'SLOW':\n",
    "        # Finding all of the slow events\n",
    "        events_boolean = base_events['rectime'] > median(base_events['rectime'])\n",
    "        \n",
    "    # Filtering events\n",
    "    rectimes_events = base_events['rectime'][events_boolean]\n",
    "    events_boolean = rectimes_events > (time_length*1000) # Filtering to only fast events that are greater than a threshold\n",
    "    events_index = np.where(events_boolean)[0]\n",
    "        \n",
    "    data_z_pow_wavelet_filt = [data_z_pow_wavelet[i] for i in events_index] # Filters the array to only fast or slow events and above threshold\n",
    "    \n",
    "    # Defining the time period for time-frequency plot\n",
    "    # Structure = [Events, Electrodes, Frequency, Time]\n",
    "    data_period = np.zeros((len(events_index),len(elec_index),len(data_z_pow_wavelet[0].frequency),(time_length*samp_freq))) # Defining matrix of events, frequency, time\n",
    "\n",
    "    # Specifying either stimulus or response-locked analysis\n",
    "    if data_locked == 'STIM':\n",
    "        # Looping through events to create array of data\n",
    "        for i_event in range(0,len(data_z_pow_wavelet_filt)):\n",
    "            data_array_temp = array(data_z_pow_wavelet_filt[i_event]) # Adding data to a matrix from one electrode over multiple events\n",
    "            data_period[i_event,:,:,:] = data_array_temp[elec_index,:,0:(time_length*samp_freq)] # [Events, Electrodes, Frequency, Time]\n",
    "    elif data_locked == 'RESP':\n",
    "        # Looping through events to create array of data\n",
    "        for i_event in range(0,len(data_z_pow_wavelet_filt)):\n",
    "            data_array_temp = array(data_z_pow_wavelet_filt[i_event]) # Adding data to a matrix from one electrode over multiple events\n",
    "            data_period[i_event,:,:,:] = data_array_temp[elec_index,:,-(time_length*samp_freq):] # [Events, Electrodes, Frequency, Time]\n",
    "            \n",
    "    data_elec_mean = mean(data_period,axis=1) # Mean over the electrodes\n",
    "    mean_time_freq_data = mean(data_elec_mean,axis=0) # Mean over the events\n",
    "    \n",
    "    return mean_time_freq_data, data_elec_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_timefreq(timefreq_data,freqs,time_period,main_title,data_locked,colorbar_max,colorbar_label):\n",
    "    # Setting the time and frequency values for plot\n",
    "    #freqs = np.logspace(np.log10(3), np.log10(180), 8)\n",
    "#     colorbar_max = .15\n",
    "    xtick_labels = range(-3,0)\n",
    "\n",
    "\n",
    "    fig = figure()\n",
    "    ax = fig.add_subplot(111) # equivalent to (1,1,1)\n",
    "    \n",
    "    if data_locked == 'STIM':\n",
    "        cax = matshow(timefreq_data, aspect='auto', fignum=0, origin ='lower', interpolation='nearest',extent=[0,time_period,0,8],vmin=-colorbar_max,vmax=colorbar_max,cmap=cm.seismic)\n",
    "    elif data_locked == 'RESP': # Different time labeling\n",
    "        cax = matshow(timefreq_data, aspect='auto', fignum=0, origin ='lower', interpolation='nearest',extent=[-time_period,0,0,8],vmin=-colorbar_max,vmax=colorbar_max,cmap=cm.seismic)\n",
    "    \n",
    "    # Color bar attributes\n",
    "    cbar = fig.colorbar(cax)\n",
    "    cbar.set_label(colorbar_label, rotation=270,fontsize=12,labelpad=15)\n",
    "    \n",
    "    tick_freq = np.round(freqs) # Rounding frequency to whole numbers for labels\n",
    "\n",
    "    # Y axis \n",
    "    T=arange(len(tick_freq))+0.5 # Moves the major ticks a half step up\n",
    "    ax.set_yticks(T)\n",
    "    ax.set_yticklabels(tick_freq)\n",
    "    ylabel('Frequency (Hz)',fontsize=12)\n",
    "\n",
    "    # X axis\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    xlabel('Time (s)',fontsize=12)\n",
    "    \n",
    "    title(main_title,fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_calculator(data):\n",
    "    # Calculating Accuracy, Input: event structure, Output: Accuracy in double format from 0 to 1 \n",
    "    i_math_probs_correct = (data['iscorrect'] == 1)\n",
    "\n",
    "    math_probs_correct = data[i_math_probs_correct]\n",
    "\n",
    "    math_accuracy = float(math_probs_correct.shape[0])/float(data.shape[0])\n",
    "    \n",
    "    return math_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_block_plot(data,cbar_label,range_vmin,range_vmax):\n",
    "    fig = figure(figsize=(8,6))\n",
    "\n",
    "    for x_value in range(1,10):\n",
    "        # Separating accuracy by x value\n",
    "        x_value_plot = data[(x_value-1)*81:(x_value-1)*81+81]\n",
    "        x_value_plot_reshape = np.reshape(x_value_plot,[9,9])\n",
    "\n",
    "        # Plot the combination results for accuracy\n",
    "        axes = plt.subplot(3,3,x_value)\n",
    "        im = pcolor(flipud(x_value_plot_reshape),vmin=range_vmin,vmax=range_vmax,cmap='jet') # jet_r for colorbar reversed or use seismic for r/b\n",
    "        plt.title('%s+y+z'%(x_value))\n",
    "        plt.tight_layout()\n",
    "        axes.set_yticklabels(list('987654321'))\n",
    "        axes.set_xticklabels(list('123456789'))\n",
    "        \n",
    "        # Setting the x and y ticks in the middle of the box\n",
    "        axes.set_xticks(np.arange(x_value_plot_reshape.shape[0])+0.5)\n",
    "        axes.set_yticks(np.arange(x_value_plot_reshape.shape[1])+0.5)\n",
    "\n",
    "\n",
    "    plt.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig.add_axes([0.85,.15,.05,.7]) # Adds an area to place the color bar\n",
    "    cbar = fig.colorbar(im,cax=cbar_ax)\n",
    "    cbar.set_label(cbar_label,rotation=270,labelpad=20,fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_block_plot_upd(data,cbar_label,range_vmin,range_vmax):\n",
    "    # 12/2017: Changed for manuscript figure - adjusted labels and font sizes\n",
    "    fig = figure(figsize=(8,6))\n",
    "\n",
    "    for x_value in range(1,10):\n",
    "        # Separating accuracy by x value\n",
    "        x_value_plot = data[(x_value-1)*81:(x_value-1)*81+81]\n",
    "        x_value_plot_reshape = np.reshape(x_value_plot,[9,9])\n",
    "\n",
    "        # Plot the combination results for accuracy\n",
    "        axes = plt.subplot(3,3,x_value)\n",
    "        im = pcolor(flipud(x_value_plot_reshape),vmin=range_vmin,vmax=range_vmax,cmap='jet') # jet_r for colorbar reversed or use seismic for r/b\n",
    "        plt.title('%s+y+z'%(x_value),fontsize=18)\n",
    "        plt.tight_layout(pad=0.5)\n",
    "        \n",
    "        # Adjusts labels for each plot\n",
    "        if (x_value == 1) or (x_value == 4):\n",
    "#             axes.set_yticklabels(list(['','8','','6','','4','','2','']),fontsize=16)\n",
    "            axes.set_yticklabels(list(['9','','7','','5','','3','','1']),fontsize=18)\n",
    "            axes.set_xticklabels(list(['','','','','','','','','']))\n",
    "        elif (x_value == 8) or (x_value == 9):\n",
    "#             axes.set_xticklabels(list(['','2','','4','','6','','8','']),fontsize=16)\n",
    "            axes.set_xticklabels(list(['1','','3','','5','','7','','9']),fontsize=18)\n",
    "            axes.set_yticklabels(list(['','','','','','','','','']))\n",
    "        elif x_value == 7:\n",
    "#             axes.set_xticklabels(list(['','2','','4','','6','','8','']),fontsize=16)\n",
    "#             axes.set_yticklabels(list(['','8','','6','','4','','2','']),fontsize=16)\n",
    "            axes.set_xticklabels(list(['1','','3','','5','','7','','9']),fontsize=18)\n",
    "            axes.set_yticklabels(list(['9','','7','','5','','3','','1']),fontsize=18)\n",
    "        else: \n",
    "            axes.set_yticklabels(list(['','','','','','','','','']))\n",
    "            axes.set_xticklabels(list(['','','','','','','','','']))\n",
    "\n",
    "        # Setting the x and y ticks in the middle of the box\n",
    "        axes.set_xticks(np.arange(x_value_plot_reshape.shape[0])+0.5)\n",
    "        axes.set_yticks(np.arange(x_value_plot_reshape.shape[1])+0.5)\n",
    "    \n",
    "    plt.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig.add_axes([0.82,.15,.05,.7]) # Adds an area to place the color bar\n",
    "    cbar = fig.colorbar(im,cax=cbar_ax)\n",
    "    cbar.set_label(cbar_label,rotation=270,labelpad=20,fontsize=18)\n",
    "    cbar.ax.tick_params(labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fdr_orig(pvalue_list,q_value):\n",
    "### FDR Calculation\n",
    "# pvalue_list = single column list of p-values\n",
    "# q_value = desired significance level\n",
    "\n",
    "    # q_value = 0.05 # Function input\n",
    "\n",
    "    p = np.sort(np.squeeze(pvalue_list))\n",
    "    V = np.array(len(pvalue_list_sort)).astype(float)\n",
    "    I = np.array(range(0,V)).astype(float)\n",
    "\n",
    "    cVN = sum(1./(I+1))\n",
    "\n",
    "    pID = p[max(np.where(p<=(I/V*q_value))[0])]\n",
    "\n",
    "    pN = p[max(np.where(p<=(I/V*q_value/cVN))[0])]\n",
    "\n",
    "    return pID, pN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def fdr(pvalue_list,q_value):\n",
    "### FDR Calculation\n",
    "# pvalue_list = single column list of p-values\n",
    "# q_value = desired significance level\n",
    "\n",
    "    # q_value = 0.05 # Function input\n",
    "\n",
    "    p = np.sort(np.squeeze(pvalue_list))\n",
    "    V = np.array(len(pvalue_list)).astype(float)\n",
    "    I = np.array(range(1,(V+1).astype(int))).astype(float) # Changed range values\n",
    "\n",
    "    cVN = sum(1./(I))\n",
    "\n",
    "    pID = p[max(np.where(p<=(I/V*q_value))[0])]\n",
    "\n",
    "    try: # Added try clause since it could give error\n",
    "        pN = p[max(np.where(p<=(I/V*q_value/cVN))[0])]\n",
    "    except:\n",
    "        pN = 0 # there will be no significant points\n",
    "\n",
    "    return pID, pN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_NaN_subs(timefreq_ROI):\n",
    "    ## Removing subjects without time-frequency data - for timing analysis\n",
    "    # Input: list of ROIs with array of [subjects,frequency,time]\n",
    "    timefreq_ROI_del = []\n",
    "    \n",
    "    for index, ROI in enumerate(ROI_list):\n",
    "        timefreq_ROI_del.append(np.delete(timefreq_ROI[index],np.unique(np.where(isnan(timefreq_ROI[index]))[0]),0))\n",
    "        \n",
    "    return timefreq_ROI_del"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PTSA",
   "language": "python",
   "name": "ptsa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
