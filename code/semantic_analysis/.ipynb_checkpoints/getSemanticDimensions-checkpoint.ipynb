{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to get semantic values from word2vec run the first 2 cells here\n",
    "# JS 2020-10-20\n",
    "# based on semantic_dimensions.ipynb from Ethan Solomon\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('/home1/esolo/word2vec/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dictionary of all possible words in task and their 300 length word2vecs\n",
    "\n",
    "## iEEG_FR_nouns.txt and wordpool.txt get you 390 unique words.\n",
    "## I've found 461 unique words going through FR1 (after removing Spanish words)\n",
    "\n",
    "exp = 'catFR1' # 'FR1'\n",
    "\n",
    "import numpy as np\n",
    "wordpool1 = open(exp+'_english_words.txt', 'r').readlines() \n",
    "wordpool1 = [w[:-1] for w in wordpool1]; \n",
    "if exp == 'FR1':\n",
    "    wordpool1[8] = 'AX' # AX not in word2vec\n",
    "elif exp == 'catFR1':\n",
    "    wordpool1[5] = 'AX'\n",
    "\n",
    "wordpool_feats = {}\n",
    "for w in wordpool1:\n",
    "    wordpool_feats[w] = model[w.lower()]\n",
    "    \n",
    "# wordpool2 = open('wordpool.txt', 'r').readlines()\n",
    "# wordpool2 = [w[:-1] for w in wordpool2]; \n",
    "# for w in wordpool2:\n",
    "#     wordpool_feats[w] = model[w.lower()]\n",
    "\n",
    "import pickle as pk\n",
    "pk.dump(wordpool_feats, open(exp+'_wordpool_feats.pk', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07128906, -0.07373047,  0.19921875, -0.06982422, -0.17089844,\n",
       "       -0.06542969,  0.05371094, -0.40234375,  0.19628906,  0.08007812,\n",
       "        0.0177002 , -0.07714844, -0.12207031, -0.17675781, -0.12597656,\n",
       "       -0.00515747, -0.0456543 ,  0.00726318, -0.02746582,  0.00747681,\n",
       "        0.03039551,  0.16699219,  0.09521484,  0.19726562,  0.17578125,\n",
       "       -0.2890625 , -0.00646973, -0.03271484, -0.14648438,  0.01916504,\n",
       "       -0.07910156,  0.19921875,  0.0168457 , -0.06640625,  0.03442383,\n",
       "        0.07470703, -0.03417969,  0.14550781,  0.05908203,  0.10644531,\n",
       "       -0.10351562, -0.33789062, -0.02416992, -0.05029297, -0.02709961,\n",
       "       -0.06787109, -0.02502441, -0.09375   ,  0.27148438,  0.12158203,\n",
       "        0.03637695,  0.19042969, -0.01293945,  0.07861328,  0.0612793 ,\n",
       "       -0.06542969,  0.00897217,  0.03442383,  0.13964844, -0.15039062,\n",
       "       -0.23144531,  0.02392578,  0.17675781,  0.11230469,  0.01940918,\n",
       "       -0.04467773,  0.13671875, -0.00445557,  0.0703125 ,  0.10986328,\n",
       "        0.10644531, -0.08007812, -0.04296875,  0.09130859, -0.22558594,\n",
       "       -0.00952148,  0.00683594,  0.26757812, -0.02368164, -0.05615234,\n",
       "        0.265625  , -0.0222168 , -0.15429688,  0.04003906,  0.10839844,\n",
       "        0.08105469, -0.140625  , -0.28710938, -0.09130859,  0.20214844,\n",
       "        0.10449219, -0.015625  ,  0.03417969,  0.02868652, -0.02416992,\n",
       "        0.08691406,  0.10253906,  0.10546875,  0.18261719,  0.08251953,\n",
       "       -0.10302734, -0.1484375 , -0.06494141,  0.05786133,  0.3671875 ,\n",
       "       -0.05126953, -0.03808594,  0.06542969, -0.01879883, -0.11767578,\n",
       "       -0.13671875, -0.30078125,  0.08154297, -0.12207031, -0.18554688,\n",
       "        0.16699219,  0.01928711,  0.05053711,  0.003479  , -0.08056641,\n",
       "       -0.19921875, -0.08349609, -0.14453125, -0.08398438, -0.00134277,\n",
       "       -0.20117188,  0.00650024, -0.07519531,  0.17285156,  0.14746094,\n",
       "       -0.05053711, -0.16210938, -0.23242188,  0.03271484,  0.03027344,\n",
       "        0.02746582,  0.09082031,  0.08349609, -0.09863281,  0.02172852,\n",
       "       -0.00239563,  0.0703125 ,  0.10644531, -0.03881836, -0.02990723,\n",
       "        0.11767578, -0.09521484, -0.15820312,  0.26171875,  0.06640625,\n",
       "        0.05053711,  0.15625   , -0.05004883,  0.2890625 ,  0.16308594,\n",
       "       -0.06079102, -0.05957031, -0.01342773,  0.14941406, -0.05273438,\n",
       "       -0.22460938,  0.05249023,  0.07324219, -0.12060547,  0.25585938,\n",
       "       -0.1953125 ,  0.00811768, -0.07177734,  0.1328125 ,  0.06201172,\n",
       "        0.07714844,  0.05957031, -0.08886719, -0.03198242,  0.10009766,\n",
       "       -0.23144531,  0.13574219, -0.18652344,  0.046875  ,  0.16601562,\n",
       "       -0.07958984, -0.13378906,  0.01281738, -0.01177979, -0.30664062,\n",
       "        0.10888672,  0.13378906,  0.0390625 ,  0.07470703,  0.26953125,\n",
       "        0.25390625, -0.13671875, -0.13671875,  0.24707031, -0.09912109,\n",
       "       -0.02709961, -0.06445312,  0.08007812,  0.11816406,  0.09716797,\n",
       "       -0.07519531,  0.06494141,  0.07177734,  0.07666016,  0.078125  ,\n",
       "       -0.12695312,  0.02600098,  0.125     ,  0.0168457 , -0.14453125,\n",
       "       -0.15722656,  0.00592041, -0.03857422,  0.06347656,  0.10009766,\n",
       "       -0.00131989, -0.03173828, -0.16015625,  0.0703125 ,  0.01855469,\n",
       "        0.09716797,  0.07421875, -0.03393555, -0.03662109, -0.07666016,\n",
       "        0.03613281,  0.13378906, -0.02929688,  0.02526855,  0.0213623 ,\n",
       "       -0.01416016, -0.27929688, -0.21777344,  0.07910156, -0.06103516,\n",
       "        0.01031494, -0.19433594, -0.08300781, -0.16210938, -0.04833984,\n",
       "        0.05322266,  0.02197266, -0.18652344, -0.08837891, -0.03100586,\n",
       "       -0.11328125, -0.10742188, -0.02990723,  0.10693359, -0.12695312,\n",
       "       -0.01818848,  0.06298828, -0.14941406, -0.25195312,  0.02819824,\n",
       "        0.13183594, -0.30859375,  0.12402344,  0.06103516, -0.09667969,\n",
       "        0.15332031,  0.07617188,  0.11962891, -0.00769043,  0.02160645,\n",
       "        0.22949219, -0.00300598,  0.1640625 ,  0.01586914,  0.12304688,\n",
       "       -0.13964844,  0.05493164,  0.12890625,  0.02050781, -0.03051758,\n",
       "       -0.14746094,  0.07861328, -0.05957031,  0.00582886,  0.11035156,\n",
       "        0.07714844,  0.18359375,  0.05322266,  0.09130859,  0.08935547,\n",
       "        0.30859375, -0.14746094,  0.05639648,  0.17089844,  0.04980469,\n",
       "       -0.13476562,  0.09912109, -0.04541016,  0.06542969,  0.02539062,\n",
       "        0.04443359,  0.38867188,  0.05053711,  0.0177002 ,  0.11523438],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpool_feats['HEAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy\n",
    "from gensim.models import KeyedVectors\n",
    "from cmlreaders import CMLReader, get_data_index\n",
    "import itertools\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import zscore\n",
    "from scipy.spatial.distance import euclidean\n",
    "import os\n",
    "import pickle as pk\n",
    "df = get_data_index(\"r1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot convert the series to <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7993ffa0b751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Get semantic cluster transition pairwise values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m serial_pos = [int(list_words[list_words['item_name']==w]['serialpos'])-1 \n\u001b[0;32m---> 41\u001b[0;31m               \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrec_evs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m              ] # serialpos starting at 0\n\u001b[1;32m     43\u001b[0m \u001b[0mserial_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeats_removed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_repeats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserial_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7993ffa0b751>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Get semantic cluster transition pairwise values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m serial_pos = [int(list_words[list_words['item_name']==w]['serialpos'])-1 \n\u001b[0;32m---> 41\u001b[0;31m               \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrec_evs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m              ] # serialpos starting at 0\n\u001b[1;32m     43\u001b[0m \u001b[0mserial_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeats_removed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_repeats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserial_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env1/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot convert the series to \"\u001b[0m \u001b[0;34m\"{0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"__{name}__\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert the series to <class 'int'>"
     ]
    }
   ],
   "source": [
    "## Example subject semantic clustering \n",
    "\n",
    "model = pk.load(open('/home1/esolo/notebooks/Semantic_dimensions/wordpool_feats.pk', 'rb')) # dictionary of words\n",
    "\n",
    "#Load subjects information\n",
    "arg = ['R1425D','FR1',1] #['R1001P', 'FR1', 1]\n",
    "s = arg[0]\n",
    "exp = arg[1]\n",
    "sess = arg[2]\n",
    "loc = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['localization'])\n",
    "mont = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['montage'])\n",
    "\n",
    "sessions = df[np.logical_and(df[\"subject\"] == s, df['experiment']==exp)]['session'].unique()\n",
    "\n",
    "#Get task eveents\n",
    "reader = CMLReader(s, exp, sess, montage=mont, localization=loc)\n",
    "evs = reader.load(\"events\")\n",
    "word_evs = evs[evs['type']=='WORD']\n",
    "\n",
    "ndim = 1 # number of PC dimensions (Ethan usually found only 1 worked for theta/FC)\n",
    "listnum = 1 # select list to look at for this sessions\n",
    "\n",
    "#Get info from one list\n",
    "list_words = word_evs[word_evs['list']==listnum]\n",
    "words = list(list_words['item_name'])\n",
    "if 'AXE' in words:\n",
    "    list_words = list_words.replace('AXE','AX')\n",
    "\n",
    "#Project semantic features for this list to 1 dimension\n",
    "feats = np.array([model[w] \n",
    "                  for w in list_words['item_name']\n",
    "                 ])  #construct feature matrix from one list; # WORDs X 300 vecs\n",
    "pca = PCA(n_components=1)\n",
    "pcs = pca.fit_transform(feats) # list of ndim PCs for 12 words\n",
    "\n",
    "# get recalls\n",
    "rec_evs = evs[(evs['type']=='REC_WORD') & (evs['list']==listnum) & (evs['intrusion']==0)]\n",
    "\n",
    "# Get semantic cluster transition pairwise values \n",
    "serial_pos = [int(list_words[list_words['item_name']==w]['serialpos'])-1 \n",
    "              for w in rec_evs['item_name']\n",
    "             ] # serialpos starting at 0\n",
    "serial_pos, repeats_removed = remove_repeats(serial_pos)\n",
    "semantic_transition_scores = get_recall_clustering(pcs, serial_pos)\n",
    "print('Semantic:')\n",
    "semantic_transition_scores\n",
    "\n",
    "# get temporal pairwise transition scores\n",
    "temporal_transition_scores = get_recall_clustering(np.arange(len(pcs)), serial_pos)\n",
    "print('Temporal:')\n",
    "temporal_transition_scores\n",
    "print('0-indexed serial position:')\n",
    "serial_pos\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 7])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serial_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall_clustering(recall_cluster_values, recall_serial_pos):\n",
    "    from scipy.stats import percentileofscore\n",
    "    #Get temporal/semantic clustering scores. \n",
    "\n",
    "    #recall_cluster_values: array of semantic/temporal values\n",
    "    #recall_serial_pos: array of indices for true recall sequence (indexing depends on when called), e.g. [1, 12, 3, 5, 9, 6]\n",
    "\n",
    "    recall_cluster_values = copy(np.array(recall_cluster_values).astype(float))\n",
    "    all_pcts = []\n",
    "    all_possible_trans = list(itertools.combinations(range(len(recall_cluster_values)), 2))\n",
    "\n",
    "    for ridx in np.arange(len(recall_serial_pos)-1):  #Loops through each recall event, except last one\n",
    "        possible_trans = [comb \n",
    "                          for comb in all_possible_trans \n",
    "                          if (recall_serial_pos[ridx] in comb)\n",
    "                         ]\n",
    "        dists = []\n",
    "        for c in possible_trans: # all possible trans within list\n",
    "            try:\n",
    "                dists.append(euclidean(recall_cluster_values[c[0]], recall_cluster_values[c[1]]))\n",
    "            except:\n",
    "                #If we did this transition, then it's a NaN, so append a NaN\n",
    "                dists.append(np.nan)\n",
    "        dists = np.array(dists)\n",
    "        dists = dists[np.isfinite(dists)]\n",
    "        true_trans = euclidean(recall_cluster_values[recall_serial_pos[ridx]], recall_cluster_values[recall_serial_pos[ridx+1]])\n",
    "        pctrank = 1.-percentileofscore(dists, true_trans, kind='strict')/100.\n",
    "        all_pcts.append(pctrank) # percentile rank within each list\n",
    "\n",
    "        recall_cluster_values[recall_serial_pos[ridx]] = np.nan\n",
    "\n",
    "    return all_pcts\n",
    "\n",
    "def remove_repeats(recall_serial_pos):\n",
    "    #Takes array of serial positions and remove second instance of a repeated word\n",
    "    items_to_keep = np.ones(len(recall_serial_pos)).astype(bool)\n",
    "    items_seen = []\n",
    "    idx_removed = []\n",
    "    for idx in range(len(recall_serial_pos)):\n",
    "        if recall_serial_pos[idx] in items_seen:\n",
    "            items_to_keep[idx] = False\n",
    "            idx_removed.append(idx)\n",
    "        items_seen.append(recall_serial_pos[idx])\n",
    "\n",
    "    final_vec = np.array(recall_serial_pos)[items_to_keep]\n",
    "    return final_vec, idx_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clustering_scores(arg, ndim=12):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pickle as pk\n",
    "    from copy import copy\n",
    "    from cmlreaders import CMLReader, get_data_index\n",
    "    import itertools\n",
    "    from sklearn.decomposition import PCA\n",
    "    from scipy.stats import zscore\n",
    "    from scipy.spatial.distance import euclidean\n",
    "    import os\n",
    "    \n",
    "#     def get_recall_clustering(positions, recalls):\n",
    "#         from scipy.stats import percentileofscore\n",
    "#         #Get temporal/semantic clustering scores. \n",
    "\n",
    "#         #Positions: array of semantic/temporal values\n",
    "#         #Recalls: array of indices for true recall sequence (zero indexed), e.g. [0, 2, 3, 5, 9, 6]\n",
    "\n",
    "#         positions = copy(np.array(positions).astype(float))\n",
    "#         all_pcts = []\n",
    "#         all_possible_trans = list(itertools.combinations(range(len(positions)), 2))\n",
    "#         for ridx in np.arange(len(recalls)-1):  #Loops through each recall event, except last one\n",
    "#             possible_trans = [comb for comb in all_possible_trans if (recalls[ridx] in comb)]\n",
    "#             dists = []\n",
    "#             for c in possible_trans:\n",
    "#                 try:\n",
    "#                     dists.append(euclidean(positions[c[0]], positions[c[1]]))\n",
    "#                 except:\n",
    "#                     #If we did this transition, then it's a NaN, so append a NaN\n",
    "#                     dists.append(np.nan)\n",
    "#             dists = np.array(dists)\n",
    "#             dists = dists[np.isfinite(dists)]\n",
    "\n",
    "#             true_trans = euclidean(positions[recalls[ridx]], positions[recalls[ridx+1]])\n",
    "#             pctrank = 1.-percentileofscore(dists, true_trans)/100.\n",
    "#             all_pcts.append(pctrank)\n",
    "\n",
    "#             positions[recalls[ridx]] = np.nan\n",
    "\n",
    "#         return np.mean(all_pcts)\n",
    "    \n",
    "#     def remove_repeats(recalls):\n",
    "#         #Takes array of serial positions and remove second instance of a repeated word\n",
    "#         items_to_keep = np.ones(len(recalls)).astype(bool)\n",
    "#         items_seen = []\n",
    "#         idx_removed = []\n",
    "#         for idx in range(len(recalls)):\n",
    "#             if recalls[idx] in items_seen:\n",
    "#                 items_to_keep[idx] = False\n",
    "#                 idx_removed.append(idx)\n",
    "#             items_seen.append(recalls[idx])\n",
    "\n",
    "#         final_vec = np.array(recalls)[items_to_keep]\n",
    "#         return final_vec, idx_removed\n",
    "    \n",
    "#     model = pk.load(open('/home1/esolo/notebooks/Semantic_dimensions/wordpool_feats.pk', 'rb'))\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #Load subjects information\n",
    "        s = arg[0]\n",
    "        exp = arg[1]\n",
    "        sess = arg[2]\n",
    "        loc = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['localization'])\n",
    "        mont = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['montage'])\n",
    "\n",
    "        sessions = df[np.logical_and(df[\"subject\"] == s, df['experiment']==exp)]['session'].unique()\n",
    "\n",
    "        #Get task eveents\n",
    "        reader = CMLReader(s, exp, sess, montage=mont, localization=loc)\n",
    "        evs = reader.load(\"events\")\n",
    "        word_evs = evs[evs['type']=='WORD']\n",
    "\n",
    "        all_trans = []\n",
    "        all_temp = []\n",
    "        all_sem = []\n",
    "\n",
    "        for i in range(251):\n",
    "            list_sem_sc = []\n",
    "            list_temp_sc = []\n",
    "\n",
    "            for listnum in word_evs['list'].unique():\n",
    "                try:\n",
    "                    #Get info from one list\n",
    "                    list_dat = word_evs[word_evs['list']==listnum]\n",
    "                    words = np.array(list_dat['item_name'])\n",
    "                    if 'AXE' in words:\n",
    "                        words[words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "\n",
    "                    #Project semantic features for this list to 1 dimension\n",
    "                    feats = np.array([model[w] for w in words])  #construct feature matrix from one list\n",
    "                    pca = PCA(n_components=ndim)\n",
    "                    pcs = pca.fit_transform(feats)\n",
    "                    #print('List '+str(listnum)+' Variance Explained: '+str(pca.explained_variance_ratio_))\n",
    "\n",
    "                    #Get recall events and their semantic values \n",
    "                    rec_evs = evs[(evs['type']=='REC_WORD') & (evs['list']==listnum) & (evs['intrusion']==0)]\n",
    "                    if len(rec_evs)<3: #don't use lists with fewer than 3 recalls\n",
    "                        continue\n",
    "                    serial_pos = [int(list_dat[list_dat['item_name']==w]['serialpos'])-1 for w in rec_evs['item_name']]\n",
    "                    serial_pos, repeats_removed = remove_repeats(serial_pos)\n",
    "\n",
    "                    #Get temporal and semantic clustering scores\n",
    "\n",
    "                    #Semantic clustering, randomly draw same number of recalls\n",
    "                    sem_sc = []\n",
    "                    foo = np.arange(12)\n",
    "                    if i == 0: \n",
    "                        list_sem_sc.append(get_recall_clustering(pcs.ravel(), serial_pos))\n",
    "                    else:\n",
    "                        np.random.shuffle(foo)\n",
    "                        tmp = foo[:len(serial_pos)]\n",
    "                        list_sem_sc.append(get_recall_clustering(pcs.ravel(), tmp))\n",
    "\n",
    "                    #Temporal clustering, shuffle the actual recall order\n",
    "#                     temp_sc = []\n",
    "#                     foo = copy(serial_pos)\n",
    "#                     if i == 0:\n",
    "#                         list_temp_sc.append(get_recall_clustering(np.arange(len(pcs)), serial_pos))\n",
    "#                     else:\n",
    "#                         np.random.shuffle(foo)\n",
    "#                         tmp = foo\n",
    "#                         list_temp_sc.append(get_recall_clustering(np.arange(len(pcs)), tmp))\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            all_temp.append(list_temp_sc)\n",
    "            all_sem.append(list_sem_sc)\n",
    "            print(i)\n",
    "\n",
    "        #Create new directories if needed\n",
    "        try:\n",
    "            os.mkdir('/scratch/esolo/grids/'+s+'/')\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            os.mkdir('/scratch/esolo/grids/'+s+'/'+str(sess)+'/')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        #Save full output\n",
    "        #np.save('/scratch/esolo/grids/'+s+'/'+str(sess)+'/temporal_clustering.npy', np.array(all_temp))\n",
    "        np.save('/scratch/esolo/grids/'+s+'/'+str(sess)+'/semantic_clustering_'+str(ndim)+'dims.npy', np.array(all_sem)) \n",
    "    except:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FR_subs = df[df['experiment']=='FR1']\n",
    "args = []\n",
    "for i in range(len(FR_subs)):\n",
    "    s = FR_subs.iloc()[i]['subject']\n",
    "    sess = FR_subs.iloc()[i]['session']\n",
    "    args.append([s, 'FR1', sess])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dims_par_explainedVar(arg):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pickle as pk\n",
    "    from copy import copy\n",
    "    from cmlreaders import CMLReader, get_data_index\n",
    "    import itertools\n",
    "    from sklearn.decomposition import PCA\n",
    "    from scipy.stats import zscore\n",
    "    from scipy.spatial.distance import euclidean\n",
    "    import os\n",
    "    \n",
    "    df = get_data_index(\"r1\")\n",
    "    model = pk.load(open('/home1/esolo/notebooks/Semantic_dimensions/wordpool_feats.pk', 'rb'))\n",
    "    all_explainedVar = []\n",
    "    \n",
    "    for d_ in [25]:\n",
    "        \n",
    "        ndim = d_\n",
    "            \n",
    "        try:\n",
    "        \n",
    "            #Load subjects information\n",
    "            s = arg[0]\n",
    "            exp = arg[1]\n",
    "            sess = arg[2]\n",
    "            loc = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['localization'])\n",
    "            mont = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['montage'])\n",
    "\n",
    "            sessions = df[np.logical_and(df[\"subject\"] == s, df['experiment']==exp)]['session'].unique()\n",
    "\n",
    "            #Get task eveents\n",
    "            reader = CMLReader(s, exp, sess, montage=mont, localization=loc)\n",
    "            evs = reader.load(\"events\")\n",
    "            word_evs = evs[evs['type']=='WORD']\n",
    "\n",
    "            #Get PCA dims for *session-level* wordpool\n",
    "            words = np.array(word_evs['item_name'])\n",
    "            if 'AXE' in words:\n",
    "                words[words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "            word_mat = np.array([model[w] for w in words])\n",
    "            pca = PCA(n_components=ndim)\n",
    "            pcs = pca.fit_transform(word_mat)\n",
    "            explainedVar = pca.explained_variance_ratio_\n",
    "            all_explainedVar.append(copy(explainedVar))\n",
    "            \n",
    "        except:\n",
    "            return\n",
    "        \n",
    "    \n",
    "#         #Load subjects information\n",
    "#         s = arg[0]\n",
    "#         exp = arg[1]\n",
    "#         sess = arg[2]\n",
    "#         loc = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['localization'])\n",
    "#         mont = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['montage'])\n",
    "\n",
    "#         sessions = df[np.logical_and(df[\"subject\"] == s, df['experiment']==exp)]['session'].unique()\n",
    "\n",
    "#         #Get task eveents\n",
    "#         reader = CMLReader(s, exp, sess, montage=mont, localization=loc)\n",
    "#         evs = reader.load(\"events\")\n",
    "#         word_evs = evs[evs['type']=='WORD']    \n",
    "    \n",
    "#         for listnum in word_evs['list'].unique():\n",
    "#             try:\n",
    "#                 #Get info from one list\n",
    "#                 list_dat = word_evs[word_evs['list']==listnum]\n",
    "#                 words = np.array(list_dat['item_name'])\n",
    "#                 if 'AXE' in words:\n",
    "#                     words[words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "\n",
    "#                 #Project semantic features for this list to 1 dimension\n",
    "#                 feats = np.array([model[w] for w in words])  #construct feature matrix from one list\n",
    "#                 pca = PCA(n_components=ndim)\n",
    "#                 pcs = pca.fit_transform(feats)\n",
    "#                 explainedVar = pca.explained_variance_ratio_\n",
    "#                 all_explainedVar.append(copy(explainedVar))\n",
    "                \n",
    "#             except:\n",
    "#                 continue\n",
    "                \n",
    "    np.save('/scratch/esolo/Semantic_dimensions/'+s+'/'+str(sess)+'/explainedVar_session.npy', np.array(all_explainedVar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277 Engines running\n",
      "Sending a shutdown signal to the controller and engines.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 16] Device or resource busy: '.nfs0000002601db588200000017'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-368965caa027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mcluster_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sge\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RAM.q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcores_per_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims_par_explainedVar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mcluster_view\u001b[0;34m(scheduler, queue, num_jobs, cores_per_job, profile, start_wait, extra_params, retries, direct, wait_for_all_engines)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mcluster_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mcluster_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_nengines_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_throwaway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m                 \u001b[0mdelete_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mdelete_profile\u001b[0;34m(profile)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_to_remove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                         \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m _use_fd_functions = ({os.open, os.stat, os.unlink, os.rmdir} <=\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 16] Device or resource busy: '.nfs0000002601db588200000017'"
     ]
    }
   ],
   "source": [
    "import cluster_helper.cluster\n",
    "with cluster_helper.cluster.cluster_view(scheduler=\"sge\", queue=\"RAM.q\", num_jobs=300, cores_per_job=1) as view:\n",
    "    view.map(dims_par_explainedVar, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173 Engines running\n",
      "Sending a shutdown signal to the controller and engines.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 16] Device or resource busy: '.nfs00000036019998220000001c'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4da945417a46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mcluster_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sge\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RAM.q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcores_per_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims_par_clustering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mcluster_view\u001b[0;34m(scheduler, queue, num_jobs, cores_per_job, profile, start_wait, extra_params, retries, direct, wait_for_all_engines)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mcluster_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mcluster_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_nengines_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_throwaway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m                 \u001b[0mdelete_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mdelete_profile\u001b[0;34m(profile)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_to_remove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                         \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m _use_fd_functions = ({os.open, os.stat, os.unlink, os.rmdir} <=\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 16] Device or resource busy: '.nfs00000036019998220000001c'"
     ]
    }
   ],
   "source": [
    "def dims_par_clustering(arg):\n",
    "\n",
    "    for d_ in range(1, 12):\n",
    "\n",
    "        def get_clustering_scores(arg, ndim=d_):\n",
    "\n",
    "            import numpy as np\n",
    "            import pickle as pk\n",
    "            from copy import copy\n",
    "            from cmlreaders import CMLReader, get_data_index\n",
    "            import itertools\n",
    "            from sklearn.decomposition import PCA\n",
    "            from scipy.stats import zscore\n",
    "            from scipy.spatial.distance import euclidean\n",
    "            import os\n",
    "\n",
    "            def get_recall_clustering(positions, recalls):\n",
    "                from scipy.stats import percentileofscore\n",
    "                #Get temporal/semantic clustering scores. \n",
    "\n",
    "                #Positions: array of semantic/temporal values\n",
    "                #Recalls: array of indices for true recall sequence (zero indexed), e.g. [0, 2, 3, 5, 9, 6]\n",
    "\n",
    "                positions = copy(np.array(positions).astype(float))\n",
    "                all_pcts = []\n",
    "                all_possible_trans = list(itertools.combinations(range(len(positions)), 2))\n",
    "                for ridx in np.arange(len(recalls)-1):  #Loops through each recall event, except last one\n",
    "                    possible_trans = [comb for comb in all_possible_trans if (recalls[ridx] in comb)]\n",
    "                    dists = []\n",
    "                    for c in possible_trans:\n",
    "                        try:\n",
    "                            dists.append(euclidean(positions[c[0]], positions[c[1]]))\n",
    "                        except:\n",
    "                            #If we did this transition, then it's a NaN, so append a NaN\n",
    "                            dists.append(np.nan)\n",
    "                    dists = np.array(dists)\n",
    "                    dists = dists[np.isfinite(dists)]\n",
    "\n",
    "                    true_trans = euclidean(positions[recalls[ridx]], positions[recalls[ridx+1]])\n",
    "                    pctrank = 1.-percentileofscore(dists, true_trans, kind='strict')/100.\n",
    "                    all_pcts.append(pctrank)\n",
    "\n",
    "                    positions[recalls[ridx]] = np.nan\n",
    "\n",
    "                return np.mean(all_pcts)\n",
    "\n",
    "            def remove_repeats(recalls):\n",
    "                #Takes array of serial positions and remove second instance of a repeated word\n",
    "                items_to_keep = np.ones(len(recalls)).astype(bool)\n",
    "                items_seen = []\n",
    "                idx_removed = []\n",
    "                for idx in range(len(recalls)):\n",
    "                    if recalls[idx] in items_seen:\n",
    "                        items_to_keep[idx] = False\n",
    "                        idx_removed.append(idx)\n",
    "                    items_seen.append(recalls[idx])\n",
    "\n",
    "                final_vec = np.array(recalls)[items_to_keep]\n",
    "                return final_vec, idx_removed\n",
    "            \n",
    "            def get_session_model(words, ndim):\n",
    "    \n",
    "                #Get PCA dims for *session-level* wordpool\n",
    "                words = np.array(words)\n",
    "                if 'AXE' in words:\n",
    "                    words[words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "                word_mat = np.array([model[w] for w in words])\n",
    "                pca = PCA(n_components=ndim)\n",
    "                pcs = pca.fit_transform(word_mat)\n",
    "                exp_var = pca.explained_variance_ratio_\n",
    "                new_model = {}\n",
    "                for idx, w in enumerate(words):\n",
    "                    new_model[w] = pcs[idx, :]\n",
    "\n",
    "                return new_model, exp_var\n",
    "\n",
    "            def get_session_PCs(word_evs, new_model, listnum):\n",
    "\n",
    "                list_dat = word_evs[word_evs['list']==listnum]\n",
    "                list_words = np.array(list_dat['item_name'])\n",
    "                if 'AXE' in list_words:\n",
    "                    list_words[list_words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "\n",
    "                #Get semantic positions from new_model\n",
    "                pcs = np.array([new_model[w_] for w_ in list_words])\n",
    "\n",
    "                return pcs\n",
    "\n",
    "            model = pk.load(open('/home1/esolo/notebooks/Semantic_dimensions/wordpool_feats.pk', 'rb'))\n",
    "            df = get_data_index(\"r1\")\n",
    "\n",
    "            try:\n",
    "\n",
    "                #Load subjects information\n",
    "                s = arg[0]\n",
    "                exp = arg[1]\n",
    "                sess = arg[2]\n",
    "                loc = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['localization'])\n",
    "                mont = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['montage'])\n",
    "\n",
    "                sessions = df[np.logical_and(df[\"subject\"] == s, df['experiment']==exp)]['session'].unique()\n",
    "\n",
    "                #Get task eveents\n",
    "                reader = CMLReader(s, exp, sess, montage=mont, localization=loc)\n",
    "                evs = reader.load(\"events\")\n",
    "                word_evs = evs[evs['type']=='WORD']\n",
    "                \n",
    "                #new_model, exp_var = get_session_model(np.array(word_evs['item_name']), ndim)\n",
    "\n",
    "                all_trans = []\n",
    "                all_temp = []\n",
    "                all_sem = []\n",
    "                \n",
    "                for i in range(251):\n",
    "                    list_sem_sc = []\n",
    "                    list_temp_sc = []\n",
    "                    words_done = []\n",
    "                    \n",
    "                    all_expVar = []\n",
    "                    for listnum in word_evs['list'].unique():\n",
    "                        try:\n",
    "                            #Get info from one list\n",
    "                            list_dat = word_evs[word_evs['list']==listnum]\n",
    "                            words = np.array(list_dat['item_name'])\n",
    "                            if 'AXE' in words:\n",
    "                                words[words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "\n",
    "                            #Project semantic features for this list to 1 dimension\n",
    "                            feats = np.array([model[w] for w in words])  #construct feature matrix from one list\n",
    "                            pca = PCA(n_components=ndim)\n",
    "                            pcs = pca.fit_transform(feats)\n",
    "\n",
    "                            #pcs = get_session_PCs(word_evs, new_model, listnum)\n",
    "\n",
    "                            #Get recall events and their semantic values \n",
    "                            rec_evs = evs[(evs['type']=='REC_WORD') & (evs['list']==listnum) & (evs['intrusion']==0)]\n",
    "                            if len(rec_evs)<4: #don't use lists with fewer than N recalls\n",
    "                                continue\n",
    "                            serial_pos = [int(list_dat[list_dat['item_name']==w]['serialpos'])-1 for w in rec_evs['item_name']]\n",
    "                            serial_pos, repeats_removed = remove_repeats(serial_pos)\n",
    "\n",
    "                            #Get temporal and semantic clustering scores\n",
    "\n",
    "                            #Semantic clustering, randomly draw same number of recalls\n",
    "                            sem_sc = []\n",
    "                            #foo = np.arange(12)\n",
    "                            foo = copy(serial_pos)\n",
    "                            if i == 0: \n",
    "                                list_sem_sc.append(get_recall_clustering(pcs, serial_pos))\n",
    "                            else:\n",
    "                                np.random.shuffle(foo)\n",
    "                                #tmp = foo[:len(serial_pos)]\n",
    "                                tmp = foo\n",
    "                                list_sem_sc.append(get_recall_clustering(pcs, tmp))\n",
    "\n",
    "                            #Temporal clustering, shuffle the actual recall order\n",
    "#                             temp_sc = []\n",
    "#                             foo = copy(serial_pos)\n",
    "#                             if i == 0:\n",
    "#                                 list_temp_sc.append(get_recall_clustering(np.arange(len(pcs)), serial_pos))\n",
    "#                             else:\n",
    "#                                 np.random.shuffle(foo)\n",
    "#                                 tmp = foo\n",
    "#                                 list_temp_sc.append(get_recall_clustering(np.arange(len(pcs)), tmp))\n",
    "\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                    #all_temp.append(list_temp_sc)\n",
    "                    all_sem.append(list_sem_sc)\n",
    "                    print(i)\n",
    "\n",
    "                #Create new directories if needed\n",
    "                try:\n",
    "                    os.mkdir('/scratch/esolo/Semantic_dimensions/'+s+'/')\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    os.mkdir('/scratch/esolo/Semantic_dimensions/'+s+'/'+str(sess)+'/')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                #Save full output\n",
    "                #np.save('/scratch/esolo/Semantic_dimensions/'+s+'/'+str(sess)+'/temporal_clustering.npy', np.array(all_temp))\n",
    "                np.save('/scratch/esolo/Semantic_dimensions/'+s+'/'+str(sess)+'/semantic_clustering_'+str(ndim)+'dims_min4recalls_list_altZscore.npy', np.array(all_sem)) \n",
    "            except:\n",
    "                return\n",
    "            \n",
    "        get_clustering_scores(arg)\n",
    "    \n",
    "\n",
    "import cluster_helper.cluster\n",
    "with cluster_helper.cluster.cluster_view(scheduler=\"sge\", queue=\"RAM.q\", num_jobs=300, cores_per_job=1) as view:\n",
    "    view.map(dims_par_clustering, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 Engines running\n",
      "Sending a shutdown signal to the controller and engines.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 16] Device or resource busy: '.nfs0000001a00dc700a0000001b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8bbe0fbb13c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mcluster_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sge\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RAM.q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcores_per_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims_par_clustering_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mcluster_view\u001b[0;34m(scheduler, queue, num_jobs, cores_per_job, profile, start_wait, extra_params, retries, direct, wait_for_all_engines)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mcluster_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mcluster_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_nengines_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_throwaway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m                 \u001b[0mdelete_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mdelete_profile\u001b[0;34m(profile)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_to_remove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                         \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m _use_fd_functions = ({os.open, os.stat, os.unlink, os.rmdir} <=\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 16] Device or resource busy: '.nfs0000001a00dc700a0000001b'"
     ]
    }
   ],
   "source": [
    "def dims_par_clustering_matrix(arg):\n",
    "\n",
    "    def get_clustering_scores(arg):\n",
    "\n",
    "        import numpy as np\n",
    "        import pickle as pk\n",
    "        from copy import copy\n",
    "        from cmlreaders import CMLReader, get_data_index\n",
    "        import itertools\n",
    "        from scipy.stats import zscore\n",
    "        import os\n",
    "        \n",
    "        def get_recall_clustering_matrix(positions, recalls):\n",
    "            from scipy.stats import percentileofscore\n",
    "            import itertools\n",
    "\n",
    "            #Get semantic clustering scores using a pre-defined matrix of distances (e.g. WordNet)\n",
    "\n",
    "            #Positions: matrix of word-word distances (NxN). Should already be symmetrized.\n",
    "            #Recalls: array of indices for true recall sequence (zero indexed), e.g. [0, 2, 3, 5, 9, 6]\n",
    "\n",
    "            all_pcts = []\n",
    "            all_possible_trans = list(itertools.combinations(range(positions.shape[0]), 2))\n",
    "            for ridx in np.arange(len(recalls)-1):  #Loops through each recall event, except last one\n",
    "                possible_trans = [comb for comb in all_possible_trans if (recalls[ridx] in comb)]\n",
    "                dists = []\n",
    "                for c in possible_trans:\n",
    "                    dists.append(positions[c[0], c[1]])   #could be appending a NaN, but that's okay\n",
    "                dists = np.array(dists)\n",
    "                dists = dists[np.isfinite(dists)]\n",
    "\n",
    "                true_trans = positions[recalls[ridx], recalls[ridx+1]]\n",
    "                pctrank = 1.-percentileofscore(dists, true_trans, kind='strict')/100.\n",
    "                all_pcts.append(pctrank)\n",
    "\n",
    "                positions[recalls[ridx], :] = np.nan; positions[:, recalls[ridx]] = np.nan  #NaN out cols/rows for recalled word\n",
    "\n",
    "            return np.mean(all_pcts)\n",
    "\n",
    "        def remove_repeats(recalls):\n",
    "            #Takes array of serial positions and remove second instance of a repeated word\n",
    "            items_to_keep = np.ones(len(recalls)).astype(bool)\n",
    "            items_seen = []\n",
    "            idx_removed = []\n",
    "            for idx in range(len(recalls)):\n",
    "                if recalls[idx] in items_seen:\n",
    "                    items_to_keep[idx] = False\n",
    "                    idx_removed.append(idx)\n",
    "                items_seen.append(recalls[idx])\n",
    "\n",
    "            final_vec = np.array(recalls)[items_to_keep]\n",
    "            return final_vec, idx_removed\n",
    "\n",
    "        def create_position_matrix(word_list):\n",
    "            from nltk.corpus import wordnet as wn\n",
    "\n",
    "            mat = np.empty([len(word_list), len(word_list)])\n",
    "            mat[:] = np.nan\n",
    "\n",
    "            for idx1, w1 in enumerate(word_list):\n",
    "                for idx2, w2 in enumerate(word_list):\n",
    "                    w1_str = w1.lower()\n",
    "                    w2_str = w2.lower()\n",
    "                    word1 = wn.synset(w1_str+'.n.01')\n",
    "                    word2 = wn.synset(w2_str+'.n.01')\n",
    "\n",
    "                    sim = word1.wup_similarity(word2)\n",
    "                    mat[idx1, idx2] = sim\n",
    "\n",
    "            return mat\n",
    "\n",
    "        df = get_data_index(\"r1\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            #Load subjects information\n",
    "            s = arg[0]\n",
    "            exp = arg[1]\n",
    "            sess = arg[2]\n",
    "            loc = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['localization'])\n",
    "            mont = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['montage'])\n",
    "\n",
    "            sessions = df[np.logical_and(df[\"subject\"] == s, df['experiment']==exp)]['session'].unique()\n",
    "\n",
    "            #Get task eveents\n",
    "            reader = CMLReader(s, exp, sess, montage=mont, localization=loc)\n",
    "            evs = reader.load(\"events\")\n",
    "            word_evs = evs[evs['type']=='WORD']\n",
    "\n",
    "            all_trans = []\n",
    "            all_temp = []\n",
    "            all_sem = []\n",
    "\n",
    "            for i in range(251):\n",
    "                list_sem_sc = []\n",
    "                list_temp_sc = []\n",
    "\n",
    "                all_expVar = []\n",
    "                for listnum in word_evs['list'].unique():\n",
    "                    try:\n",
    "                        #Get info from one list\n",
    "                        list_dat = word_evs[word_evs['list']==listnum]\n",
    "                        words = np.array(list_dat['item_name'])\n",
    "                        if 'AXE' in words:\n",
    "                            words[words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "\n",
    "                        pmat = create_position_matrix(words)\n",
    "\n",
    "                        #Get recall events and their semantic values \n",
    "                        rec_evs = evs[(evs['type']=='REC_WORD') & (evs['list']==listnum) & (evs['intrusion']==0)]\n",
    "                        if len(rec_evs)<4: #don't use lists with fewer than N recalls\n",
    "                            continue\n",
    "                        serial_pos = [int(list_dat[list_dat['item_name']==w]['serialpos'])-1 for w in rec_evs['item_name']]\n",
    "                        serial_pos, repeats_removed = remove_repeats(serial_pos)\n",
    "\n",
    "                        #Get temporal and semantic clustering scores\n",
    "\n",
    "                        #Semantic clustering, randomly draw same number of recalls\n",
    "                        sem_sc = []\n",
    "                        foo = np.arange(12)\n",
    "                        if i == 0: \n",
    "                            list_sem_sc.append(get_recall_clustering_matrix(pmat, serial_pos))\n",
    "                        else:\n",
    "                            np.random.shuffle(foo)\n",
    "                            tmp = foo[:len(serial_pos)]\n",
    "                            list_sem_sc.append(get_recall_clustering_matrix(pmat, tmp))\n",
    "\n",
    "    #                         #Temporal clustering, shuffle the actual recall order\n",
    "    #                         temp_sc = []\n",
    "    #                         foo = copy(serial_pos)\n",
    "    #                         if i == 0:\n",
    "    #                             list_temp_sc.append(get_recall_clustering(np.arange(len(pcs)), serial_pos))\n",
    "    #                         else:\n",
    "    #                             np.random.shuffle(foo)\n",
    "    #                             tmp = foo\n",
    "    #                             list_temp_sc.append(get_recall_clustering(np.arange(len(pcs)), tmp))\n",
    "\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                #all_temp.append(list_temp_sc)\n",
    "                all_sem.append(list_sem_sc)\n",
    "                print(i)\n",
    "\n",
    "            #Create new directories if needed\n",
    "            try:\n",
    "                os.mkdir('/scratch/esolo/grids/'+s+'/')\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                os.mkdir('/scratch/esolo/grids/'+s+'/'+str(sess)+'/')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            #Save full output\n",
    "            #np.save('/scratch/esolo/Semantic_dimensions/'+s+'/'+str(sess)+'/temporal_clustering.npy', np.array(all_temp))\n",
    "            np.save('/scratch/esolo/Semantic_dimensions/'+s+'/'+str(sess)+'/semantic_clustering_min4recalls_wordNet.npy', np.array(all_sem)) \n",
    "        except:\n",
    "            return\n",
    "\n",
    "    get_clustering_scores(arg)\n",
    "    \n",
    "\n",
    "import cluster_helper.cluster\n",
    "with cluster_helper.cluster.cluster_view(scheduler=\"sge\", queue=\"RAM.q\", num_jobs=300, cores_per_job=1) as view:\n",
    "    view.map(dims_par_clustering_matrix, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 Engines running\n",
      "Sending a shutdown signal to the controller and engines.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 16] Device or resource busy: '.nfs0000000b018a401900000077'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-ac8b63b4cc58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mcluster_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sge\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RAM.q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcores_per_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims_par_clustering_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mcluster_view\u001b[0;34m(scheduler, queue, num_jobs, cores_per_job, profile, start_wait, extra_params, retries, direct, wait_for_all_engines)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mcluster_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mcluster_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_nengines_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_throwaway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m                 \u001b[0mdelete_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mdelete_profile\u001b[0;34m(profile)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_to_remove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                         \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m _use_fd_functions = ({os.open, os.stat, os.unlink, os.rmdir} <=\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 16] Device or resource busy: '.nfs0000000b018a401900000077'"
     ]
    }
   ],
   "source": [
    "def dims_par_clustering_session(arg):\n",
    "\n",
    "    for d_ in range(14, 25):\n",
    "\n",
    "        def get_clustering_scores(arg, ndim=d_):\n",
    "\n",
    "            import numpy as np\n",
    "            import pickle as pk\n",
    "            from copy import copy\n",
    "            from cmlreaders import CMLReader, get_data_index\n",
    "            import itertools\n",
    "            from sklearn.decomposition import PCA\n",
    "            from scipy.stats import zscore\n",
    "            from scipy.spatial.distance import euclidean\n",
    "            import os\n",
    "\n",
    "            def get_recall_clustering(positions, recalls):\n",
    "                from scipy.stats import percentileofscore\n",
    "                #Get temporal/semantic clustering scores. \n",
    "\n",
    "                #Positions: array of semantic/temporal values\n",
    "                #Recalls: array of indices for true recall sequence (zero indexed), e.g. [0, 2, 3, 5, 9, 6]\n",
    "\n",
    "                positions = copy(np.array(positions).astype(float))\n",
    "                all_pcts = []\n",
    "                all_possible_trans = list(itertools.combinations(range(len(positions)), 2))\n",
    "                for ridx in np.arange(len(recalls)-1):  #Loops through each recall event, except last one\n",
    "                    possible_trans = [comb for comb in all_possible_trans if (recalls[ridx] in comb)]\n",
    "                    dists = []\n",
    "                    for c in possible_trans:\n",
    "                        try:\n",
    "                            dists.append(euclidean(positions[c[0]], positions[c[1]]))\n",
    "                        except:\n",
    "                            #If we did this transition, then it's a NaN, so append a NaN\n",
    "                            dists.append(np.nan)\n",
    "                    dists = np.array(dists)\n",
    "                    dists = dists[np.isfinite(dists)]\n",
    "\n",
    "                    true_trans = euclidean(positions[recalls[ridx]], positions[recalls[ridx+1]])\n",
    "                    pctrank = 1.-percentileofscore(dists, true_trans)/100.\n",
    "                    all_pcts.append(pctrank)\n",
    "\n",
    "                    positions[recalls[ridx]] = np.nan\n",
    "\n",
    "                return np.mean(all_pcts)\n",
    "\n",
    "            def remove_repeats(recalls):\n",
    "                #Takes array of serial positions and remove second instance of a repeated word\n",
    "                items_to_keep = np.ones(len(recalls)).astype(bool)\n",
    "                items_seen = []\n",
    "                idx_removed = []\n",
    "                for idx in range(len(recalls)):\n",
    "                    if recalls[idx] in items_seen:\n",
    "                        items_to_keep[idx] = False\n",
    "                        idx_removed.append(idx)\n",
    "                    items_seen.append(recalls[idx])\n",
    "\n",
    "                final_vec = np.array(recalls)[items_to_keep]\n",
    "                return final_vec, idx_removed\n",
    "\n",
    "            model = pk.load(open('/home1/esolo/notebooks/Semantic_dimensions/wordpool_feats.pk', 'rb'))\n",
    "            df = get_data_index(\"r1\")\n",
    "\n",
    "            try:\n",
    "\n",
    "                #Load subjects information\n",
    "                s = arg[0]\n",
    "                exp = arg[1]\n",
    "                sess = arg[2]\n",
    "                loc = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['localization'])\n",
    "                mont = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['montage'])\n",
    "\n",
    "                sessions = df[np.logical_and(df[\"subject\"] == s, df['experiment']==exp)]['session'].unique()\n",
    "\n",
    "                #Get task eveents\n",
    "                reader = CMLReader(s, exp, sess, montage=mont, localization=loc)\n",
    "                evs = reader.load(\"events\")\n",
    "                word_evs = evs[evs['type']=='WORD']\n",
    "\n",
    "                all_trans = []\n",
    "                all_temp = []\n",
    "                all_sem = []\n",
    "                \n",
    "                #Get PCA dims for *session-level* wordpool\n",
    "                words = np.array(word_evs['item_name'])\n",
    "                if 'AXE' in words:\n",
    "                    words[words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "                word_mat = np.array([model[w] for w in words])\n",
    "                pca = PCA(n_components=ndim)\n",
    "                pcs = pca.fit_transform(word_mat)\n",
    "                new_model = {}\n",
    "                for idx, w in enumerate(word_evs['item_name']):\n",
    "                    new_model[w] = pcs[idx, :]\n",
    "\n",
    "                for i in range(251):\n",
    "                    list_sem_sc = []\n",
    "                    list_temp_sc = []\n",
    "\n",
    "                    for listnum in word_evs['list'].unique():\n",
    "                        try:\n",
    "\n",
    "                            list_dat = word_evs[word_evs['list']==listnum]\n",
    "                            list_words = np.array(list_dat['item_name'])\n",
    "                            if 'AXE' in list_words:\n",
    "                                list_words[list_words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "                            \n",
    "                            #Get recall events and their semantic values from one list\n",
    "                            rec_evs = evs[(evs['type']=='REC_WORD') & (evs['list']==listnum) & (evs['intrusion']==0)]\n",
    "                            if len(rec_evs)<3: #don't use lists with fewer than 3 recalls\n",
    "                                continue\n",
    "                            serial_pos = [int(list_dat[list_dat['item_name']==w]['serialpos'])-1 for w in rec_evs['item_name']]\n",
    "                            serial_pos, repeats_removed = remove_repeats(serial_pos)\n",
    "\n",
    "                            #Get semantic positions from new_model\n",
    "                            pcs = np.array([new_model[w_] for w_ in list_words])\n",
    "\n",
    "                            #Semantic clustering, randomly draw same number of recalls\n",
    "                            sem_sc = []\n",
    "                            foo = np.arange(12)\n",
    "                            if i == 0: \n",
    "                                list_sem_sc.append(get_recall_clustering(pcs, serial_pos))\n",
    "                            else:\n",
    "                                np.random.shuffle(foo)\n",
    "                                tmp = foo[:len(serial_pos)]\n",
    "                                list_sem_sc.append(get_recall_clustering(pcs, tmp))\n",
    "\n",
    "                            #Temporal clustering, shuffle the actual recall order\n",
    "        #                     temp_sc = []\n",
    "        #                     foo = copy(serial_pos)\n",
    "        #                     if i == 0:\n",
    "        #                         list_temp_sc.append(get_recall_clustering(np.arange(len(pcs)), serial_pos))\n",
    "        #                     else:\n",
    "        #                         np.random.shuffle(foo)\n",
    "        #                         tmp = foo\n",
    "        #                         list_temp_sc.append(get_recall_clustering(np.arange(len(pcs)), tmp))\n",
    "\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                    all_temp.append(list_temp_sc)\n",
    "                    all_sem.append(list_sem_sc)\n",
    "                    print(i)\n",
    "\n",
    "                #Create new directories if needed\n",
    "                try:\n",
    "                    os.mkdir('/scratch/esolo/Semantic_dimensions/'+s+'/')\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    os.mkdir('/scratch/esolo/Semantic_dimensions/'+s+'/'+str(sess)+'/')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                #Save full output\n",
    "                #np.save('/scratch/esolo/grids/'+s+'/'+str(sess)+'/temporal_clustering.npy', np.array(all_temp))\n",
    "                np.save('/scratch/esolo/Semantic_dimensions/'+s+'/'+str(sess)+'/semantic_clustering_'+str(ndim)+'dims_min3recalls_session.npy', np.array(all_sem)) \n",
    "            except:\n",
    "                return\n",
    "            \n",
    "        get_clustering_scores(arg)\n",
    "    \n",
    "\n",
    "import cluster_helper.cluster\n",
    "with cluster_helper.cluster.cluster_view(scheduler=\"sge\", queue=\"RAM.q\", num_jobs=300, cores_per_job=1) as view:\n",
    "    view.map(dims_par_clustering_session, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
