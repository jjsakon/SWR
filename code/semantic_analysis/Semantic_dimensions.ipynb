{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('/home1/esolo/word2vec/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dictionary of all possible words in task and their 300 length word2vecs\n",
    "\n",
    "## iEEG_FR_nouns.txt and wordpool.txt get you 390 unique words.\n",
    "## I've found 461 unique words going through FR1 (after removing Spanish words)\n",
    "\n",
    "import numpy as np\n",
    "wordpool1 = open('FR1_english_words.txt', 'r').readlines() # iEEG_FR_nouns.txt\n",
    "wordpool1 = [w[:-1] for w in wordpool1]; wordpool1[8]='AX'\n",
    "\n",
    "wordpool_feats = {}\n",
    "for w in wordpool1:\n",
    "    wordpool_feats[w] = model[w.lower()]\n",
    "    \n",
    "# wordpool2 = open('wordpool.txt', 'r').readlines()\n",
    "# wordpool2 = [w[:-1] for w in wordpool2]; \n",
    "# for w in wordpool2:\n",
    "#     wordpool_feats[w] = model[w.lower()]\n",
    "\n",
    "import pickle as pk\n",
    "pk.dump(wordpool_feats, open('wordpool_feats_461.pk', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(range(1, 12))\n",
    "len(range(14,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy\n",
    "from gensim.models import KeyedVectors\n",
    "from cmlreaders import CMLReader, get_data_index\n",
    "import itertools\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import zscore\n",
    "from scipy.spatial.distance import euclidean\n",
    "import os\n",
    "import pickle as pk\n",
    "df = get_data_index(\"r1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8181818181818181]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4545454545454546]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-indexed serial position:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 7])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['MULE',\n",
       " 'STORM',\n",
       " 'HEN',\n",
       " 'EGG',\n",
       " 'SKI',\n",
       " 'PEN',\n",
       " 'FORT',\n",
       " 'BEAK',\n",
       " 'SAIL',\n",
       " 'FARM',\n",
       " 'FUR',\n",
       " 'TRAIN']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example subject semantic clustering \n",
    "\n",
    "model = pk.load(open('/home1/esolo/notebooks/Semantic_dimensions/wordpool_feats.pk', 'rb')) # dictionary of words\n",
    "\n",
    "#Load subjects information\n",
    "arg = ['R1001P', 'FR1', 1]\n",
    "s = arg[0]\n",
    "exp = arg[1]\n",
    "sess = arg[2]\n",
    "loc = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['localization'])\n",
    "mont = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['montage'])\n",
    "\n",
    "sessions = df[np.logical_and(df[\"subject\"] == s, df['experiment']==exp)]['session'].unique()\n",
    "\n",
    "#Get task eveents\n",
    "reader = CMLReader(s, exp, sess, montage=mont, localization=loc)\n",
    "evs = reader.load(\"events\")\n",
    "word_evs = evs[evs['type']=='WORD']\n",
    "\n",
    "ndim = 1 # number of PC dimensions (Ethan usually found only 1 worked for theta/FC)\n",
    "listnum = 4 # select list to look at for this sessions\n",
    "\n",
    "#Get info from one list\n",
    "list_dat = word_evs[word_evs['list']==listnum]\n",
    "words = list(list_dat['item_name'])\n",
    "\n",
    "#Project semantic features for this list to 1 dimension\n",
    "feats = np.array([model[w] \n",
    "                  for w in list_dat['item_name']\n",
    "                 ])  #construct feature matrix from one list; # WORDs X 300 vecs\n",
    "pca = PCA(n_components=1)\n",
    "pcs = pca.fit_transform(feats) # list of ndim PCs for 12 words\n",
    "\n",
    "# get recalls\n",
    "rec_evs = evs[(evs['type']=='REC_WORD') & (evs['list']==listnum) & (evs['intrusion']==0)]\n",
    "\n",
    "# Get semantic cluster transition pairwise values \n",
    "serial_pos = [int(list_dat[list_dat['item_name']==w]['serialpos'])-1 \n",
    "              for w in rec_evs['item_name']\n",
    "             ] # serialpos starting at 0\n",
    "serial_pos, repeats_removed = remove_repeats(serial_pos)\n",
    "semantic_transition_scores = get_recall_clustering(pcs, serial_pos)\n",
    "print('Semantic:')\n",
    "semantic_transition_scores\n",
    "\n",
    "# get temporal pairwise transition scores\n",
    "temporal_transition_scores = get_recall_clustering(np.arange(len(pcs)), serial_pos)\n",
    "print('Temporal:')\n",
    "temporal_transition_scores\n",
    "print('0-indexed serial position:')\n",
    "serial_pos\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.35210073],\n",
       "       [-1.0532515 ],\n",
       "       [ 1.3383746 ],\n",
       "       [ 1.6368098 ],\n",
       "       [-1.5043539 ],\n",
       "       [ 0.39463693],\n",
       "       [-0.95285213],\n",
       "       [ 1.7572211 ],\n",
       "       [-1.1632018 ],\n",
       "       [ 0.01127505],\n",
       "       [ 1.1254106 ],\n",
       "       [-1.2379668 ]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall_clustering(recall_cluster_values, recall_serial_pos):\n",
    "    from scipy.stats import percentileofscore\n",
    "    #Get temporal/semantic clustering scores. \n",
    "\n",
    "    #recall_cluster_values: array of semantic/temporal values\n",
    "    #recall_serial_pos: array of indices for true recall sequence (zero indexed), e.g. [0, 2, 3, 5, 9, 6]\n",
    "\n",
    "    recall_cluster_values = copy(np.array(recall_cluster_values).astype(float))\n",
    "    all_pcts = []\n",
    "    all_possible_trans = list(itertools.combinations(range(len(recall_cluster_values)), 2))\n",
    "\n",
    "    for ridx in np.arange(len(recall_serial_pos)-1):  #Loops through each recall event, except last one\n",
    "        possible_trans = [comb \n",
    "                          for comb in all_possible_trans \n",
    "                          if (recall_serial_pos[ridx] in comb)\n",
    "                         ]\n",
    "        dists = []\n",
    "        for c in possible_trans: # all possible trans within list\n",
    "            try:\n",
    "                dists.append(euclidean(recall_cluster_values[c[0]], recall_cluster_values[c[1]]))\n",
    "            except:\n",
    "                #If we did this transition, then it's a NaN, so append a NaN\n",
    "                dists.append(np.nan)\n",
    "        dists = np.array(dists)\n",
    "        dists = dists[np.isfinite(dists)]\n",
    "        true_trans = euclidean(recall_cluster_values[recall_serial_pos[ridx]], recall_cluster_values[recall_serial_pos[ridx+1]])\n",
    "        pctrank = 1.-percentileofscore(dists, true_trans, kind='strict')/100.\n",
    "        all_pcts.append(pctrank) # percentile rank within each list\n",
    "\n",
    "        recall_cluster_values[recall_serial_pos[ridx]] = np.nan\n",
    "\n",
    "    return all_pcts\n",
    "\n",
    "def remove_repeats(recall_serial_pos):\n",
    "    #Takes array of serial positions and remove second instance of a repeated word\n",
    "    items_to_keep = np.ones(len(recall_serial_pos)).astype(bool)\n",
    "    items_seen = []\n",
    "    idx_removed = []\n",
    "    for idx in range(len(recall_serial_pos)):\n",
    "        if recall_serial_pos[idx] in items_seen:\n",
    "            items_to_keep[idx] = False\n",
    "            idx_removed.append(idx)\n",
    "        items_seen.append(recall_serial_pos[idx])\n",
    "\n",
    "    final_vec = np.array(recall_serial_pos)[items_to_keep]\n",
    "    return final_vec, idx_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clustering_scores(arg, ndim=12):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pickle as pk\n",
    "    from copy import copy\n",
    "    from cmlreaders import CMLReader, get_data_index\n",
    "    import itertools\n",
    "    from sklearn.decomposition import PCA\n",
    "    from scipy.stats import zscore\n",
    "    from scipy.spatial.distance import euclidean\n",
    "    import os\n",
    "    \n",
    "#     def get_recall_clustering(positions, recalls):\n",
    "#         from scipy.stats import percentileofscore\n",
    "#         #Get temporal/semantic clustering scores. \n",
    "\n",
    "#         #Positions: array of semantic/temporal values\n",
    "#         #Recalls: array of indices for true recall sequence (zero indexed), e.g. [0, 2, 3, 5, 9, 6]\n",
    "\n",
    "#         positions = copy(np.array(positions).astype(float))\n",
    "#         all_pcts = []\n",
    "#         all_possible_trans = list(itertools.combinations(range(len(positions)), 2))\n",
    "#         for ridx in np.arange(len(recalls)-1):  #Loops through each recall event, except last one\n",
    "#             possible_trans = [comb for comb in all_possible_trans if (recalls[ridx] in comb)]\n",
    "#             dists = []\n",
    "#             for c in possible_trans:\n",
    "#                 try:\n",
    "#                     dists.append(euclidean(positions[c[0]], positions[c[1]]))\n",
    "#                 except:\n",
    "#                     #If we did this transition, then it's a NaN, so append a NaN\n",
    "#                     dists.append(np.nan)\n",
    "#             dists = np.array(dists)\n",
    "#             dists = dists[np.isfinite(dists)]\n",
    "\n",
    "#             true_trans = euclidean(positions[recalls[ridx]], positions[recalls[ridx+1]])\n",
    "#             pctrank = 1.-percentileofscore(dists, true_trans)/100.\n",
    "#             all_pcts.append(pctrank)\n",
    "\n",
    "#             positions[recalls[ridx]] = np.nan\n",
    "\n",
    "#         return np.mean(all_pcts)\n",
    "    \n",
    "#     def remove_repeats(recalls):\n",
    "#         #Takes array of serial positions and remove second instance of a repeated word\n",
    "#         items_to_keep = np.ones(len(recalls)).astype(bool)\n",
    "#         items_seen = []\n",
    "#         idx_removed = []\n",
    "#         for idx in range(len(recalls)):\n",
    "#             if recalls[idx] in items_seen:\n",
    "#                 items_to_keep[idx] = False\n",
    "#                 idx_removed.append(idx)\n",
    "#             items_seen.append(recalls[idx])\n",
    "\n",
    "#         final_vec = np.array(recalls)[items_to_keep]\n",
    "#         return final_vec, idx_removed\n",
    "    \n",
    "#     model = pk.load(open('/home1/esolo/notebooks/Semantic_dimensions/wordpool_feats.pk', 'rb'))\n",
    "    \n",
    "    try:\n",
    "\n",
    "        #Load subjects information\n",
    "        s = arg[0]\n",
    "        exp = arg[1]\n",
    "        sess = arg[2]\n",
    "        loc = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['localization'])\n",
    "        mont = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['montage'])\n",
    "\n",
    "        sessions = df[np.logical_and(df[\"subject\"] == s, df['experiment']==exp)]['session'].unique()\n",
    "\n",
    "        #Get task eveents\n",
    "        reader = CMLReader(s, exp, sess, montage=mont, localization=loc)\n",
    "        evs = reader.load(\"events\")\n",
    "        word_evs = evs[evs['type']=='WORD']\n",
    "\n",
    "        all_trans = []\n",
    "        all_temp = []\n",
    "        all_sem = []\n",
    "\n",
    "        for i in range(251):\n",
    "            list_sem_sc = []\n",
    "            list_temp_sc = []\n",
    "\n",
    "            for listnum in word_evs['list'].unique():\n",
    "                try:\n",
    "                    #Get info from one list\n",
    "                    list_dat = word_evs[word_evs['list']==listnum]\n",
    "                    words = np.array(list_dat['item_name'])\n",
    "                    if 'AXE' in words:\n",
    "                        words[words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "\n",
    "                    #Project semantic features for this list to 1 dimension\n",
    "                    feats = np.array([model[w] for w in words])  #construct feature matrix from one list\n",
    "                    pca = PCA(n_components=ndim)\n",
    "                    pcs = pca.fit_transform(feats)\n",
    "                    #print('List '+str(listnum)+' Variance Explained: '+str(pca.explained_variance_ratio_))\n",
    "\n",
    "                    #Get recall events and their semantic values \n",
    "                    rec_evs = evs[(evs['type']=='REC_WORD') & (evs['list']==listnum) & (evs['intrusion']==0)]\n",
    "                    if len(rec_evs)<3: #don't use lists with fewer than 3 recalls\n",
    "                        continue\n",
    "                    serial_pos = [int(list_dat[list_dat['item_name']==w]['serialpos'])-1 for w in rec_evs['item_name']]\n",
    "                    serial_pos, repeats_removed = remove_repeats(serial_pos)\n",
    "\n",
    "                    #Get temporal and semantic clustering scores\n",
    "\n",
    "                    #Semantic clustering, randomly draw same number of recalls\n",
    "                    sem_sc = []\n",
    "                    foo = np.arange(12)\n",
    "                    if i == 0: \n",
    "                        list_sem_sc.append(get_recall_clustering(pcs.ravel(), serial_pos))\n",
    "                    else:\n",
    "                        np.random.shuffle(foo)\n",
    "                        tmp = foo[:len(serial_pos)]\n",
    "                        list_sem_sc.append(get_recall_clustering(pcs.ravel(), tmp))\n",
    "\n",
    "                    #Temporal clustering, shuffle the actual recall order\n",
    "#                     temp_sc = []\n",
    "#                     foo = copy(serial_pos)\n",
    "#                     if i == 0:\n",
    "#                         list_temp_sc.append(get_recall_clustering(np.arange(len(pcs)), serial_pos))\n",
    "#                     else:\n",
    "#                         np.random.shuffle(foo)\n",
    "#                         tmp = foo\n",
    "#                         list_temp_sc.append(get_recall_clustering(np.arange(len(pcs)), tmp))\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            all_temp.append(list_temp_sc)\n",
    "            all_sem.append(list_sem_sc)\n",
    "            print(i)\n",
    "\n",
    "        #Create new directories if needed\n",
    "        try:\n",
    "            os.mkdir('/scratch/esolo/grids/'+s+'/')\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            os.mkdir('/scratch/esolo/grids/'+s+'/'+str(sess)+'/')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        #Save full output\n",
    "        #np.save('/scratch/esolo/grids/'+s+'/'+str(sess)+'/temporal_clustering.npy', np.array(all_temp))\n",
    "        np.save('/scratch/esolo/grids/'+s+'/'+str(sess)+'/semantic_clustering_'+str(ndim)+'dims.npy', np.array(all_sem)) \n",
    "    except:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FR_subs = df[df['experiment']=='FR1']\n",
    "args = []\n",
    "for i in range(len(FR_subs)):\n",
    "    s = FR_subs.iloc()[i]['subject']\n",
    "    sess = FR_subs.iloc()[i]['session']\n",
    "    args.append([s, 'FR1', sess])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dims_par_explainedVar(arg):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pickle as pk\n",
    "    from copy import copy\n",
    "    from cmlreaders import CMLReader, get_data_index\n",
    "    import itertools\n",
    "    from sklearn.decomposition import PCA\n",
    "    from scipy.stats import zscore\n",
    "    from scipy.spatial.distance import euclidean\n",
    "    import os\n",
    "    \n",
    "    df = get_data_index(\"r1\")\n",
    "    model = pk.load(open('/home1/esolo/notebooks/Semantic_dimensions/wordpool_feats.pk', 'rb'))\n",
    "    all_explainedVar = []\n",
    "    \n",
    "    for d_ in [25]:\n",
    "        \n",
    "        ndim = d_\n",
    "            \n",
    "        try:\n",
    "        \n",
    "            #Load subjects information\n",
    "            s = arg[0]\n",
    "            exp = arg[1]\n",
    "            sess = arg[2]\n",
    "            loc = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['localization'])\n",
    "            mont = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['montage'])\n",
    "\n",
    "            sessions = df[np.logical_and(df[\"subject\"] == s, df['experiment']==exp)]['session'].unique()\n",
    "\n",
    "            #Get task eveents\n",
    "            reader = CMLReader(s, exp, sess, montage=mont, localization=loc)\n",
    "            evs = reader.load(\"events\")\n",
    "            word_evs = evs[evs['type']=='WORD']\n",
    "\n",
    "            #Get PCA dims for *session-level* wordpool\n",
    "            words = np.array(word_evs['item_name'])\n",
    "            if 'AXE' in words:\n",
    "                words[words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "            word_mat = np.array([model[w] for w in words])\n",
    "            pca = PCA(n_components=ndim)\n",
    "            pcs = pca.fit_transform(word_mat)\n",
    "            explainedVar = pca.explained_variance_ratio_\n",
    "            all_explainedVar.append(copy(explainedVar))\n",
    "            \n",
    "        except:\n",
    "            return\n",
    "        \n",
    "    \n",
    "#         #Load subjects information\n",
    "#         s = arg[0]\n",
    "#         exp = arg[1]\n",
    "#         sess = arg[2]\n",
    "#         loc = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['localization'])\n",
    "#         mont = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['montage'])\n",
    "\n",
    "#         sessions = df[np.logical_and(df[\"subject\"] == s, df['experiment']==exp)]['session'].unique()\n",
    "\n",
    "#         #Get task eveents\n",
    "#         reader = CMLReader(s, exp, sess, montage=mont, localization=loc)\n",
    "#         evs = reader.load(\"events\")\n",
    "#         word_evs = evs[evs['type']=='WORD']    \n",
    "    \n",
    "#         for listnum in word_evs['list'].unique():\n",
    "#             try:\n",
    "#                 #Get info from one list\n",
    "#                 list_dat = word_evs[word_evs['list']==listnum]\n",
    "#                 words = np.array(list_dat['item_name'])\n",
    "#                 if 'AXE' in words:\n",
    "#                     words[words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "\n",
    "#                 #Project semantic features for this list to 1 dimension\n",
    "#                 feats = np.array([model[w] for w in words])  #construct feature matrix from one list\n",
    "#                 pca = PCA(n_components=ndim)\n",
    "#                 pcs = pca.fit_transform(feats)\n",
    "#                 explainedVar = pca.explained_variance_ratio_\n",
    "#                 all_explainedVar.append(copy(explainedVar))\n",
    "                \n",
    "#             except:\n",
    "#                 continue\n",
    "                \n",
    "    np.save('/scratch/esolo/Semantic_dimensions/'+s+'/'+str(sess)+'/explainedVar_session.npy', np.array(all_explainedVar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277 Engines running\n",
      "Sending a shutdown signal to the controller and engines.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 16] Device or resource busy: '.nfs0000002601db588200000017'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-368965caa027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mcluster_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sge\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RAM.q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcores_per_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims_par_explainedVar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mcluster_view\u001b[0;34m(scheduler, queue, num_jobs, cores_per_job, profile, start_wait, extra_params, retries, direct, wait_for_all_engines)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mcluster_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mcluster_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_nengines_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_throwaway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m                 \u001b[0mdelete_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mdelete_profile\u001b[0;34m(profile)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_to_remove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                         \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m _use_fd_functions = ({os.open, os.stat, os.unlink, os.rmdir} <=\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 16] Device or resource busy: '.nfs0000002601db588200000017'"
     ]
    }
   ],
   "source": [
    "import cluster_helper.cluster\n",
    "with cluster_helper.cluster.cluster_view(scheduler=\"sge\", queue=\"RAM.q\", num_jobs=300, cores_per_job=1) as view:\n",
    "    view.map(dims_par_explainedVar, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173 Engines running\n",
      "Sending a shutdown signal to the controller and engines.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 16] Device or resource busy: '.nfs00000036019998220000001c'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4da945417a46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mcluster_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sge\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RAM.q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcores_per_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims_par_clustering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mcluster_view\u001b[0;34m(scheduler, queue, num_jobs, cores_per_job, profile, start_wait, extra_params, retries, direct, wait_for_all_engines)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mcluster_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mcluster_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_nengines_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_throwaway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m                 \u001b[0mdelete_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mdelete_profile\u001b[0;34m(profile)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_to_remove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                         \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m _use_fd_functions = ({os.open, os.stat, os.unlink, os.rmdir} <=\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 16] Device or resource busy: '.nfs00000036019998220000001c'"
     ]
    }
   ],
   "source": [
    "def dims_par_clustering(arg):\n",
    "\n",
    "    for d_ in range(1, 12):\n",
    "\n",
    "        def get_clustering_scores(arg, ndim=d_):\n",
    "\n",
    "            import numpy as np\n",
    "            import pickle as pk\n",
    "            from copy import copy\n",
    "            from cmlreaders import CMLReader, get_data_index\n",
    "            import itertools\n",
    "            from sklearn.decomposition import PCA\n",
    "            from scipy.stats import zscore\n",
    "            from scipy.spatial.distance import euclidean\n",
    "            import os\n",
    "\n",
    "            def get_recall_clustering(positions, recalls):\n",
    "                from scipy.stats import percentileofscore\n",
    "                #Get temporal/semantic clustering scores. \n",
    "\n",
    "                #Positions: array of semantic/temporal values\n",
    "                #Recalls: array of indices for true recall sequence (zero indexed), e.g. [0, 2, 3, 5, 9, 6]\n",
    "\n",
    "                positions = copy(np.array(positions).astype(float))\n",
    "                all_pcts = []\n",
    "                all_possible_trans = list(itertools.combinations(range(len(positions)), 2))\n",
    "                for ridx in np.arange(len(recalls)-1):  #Loops through each recall event, except last one\n",
    "                    possible_trans = [comb for comb in all_possible_trans if (recalls[ridx] in comb)]\n",
    "                    dists = []\n",
    "                    for c in possible_trans:\n",
    "                        try:\n",
    "                            dists.append(euclidean(positions[c[0]], positions[c[1]]))\n",
    "                        except:\n",
    "                            #If we did this transition, then it's a NaN, so append a NaN\n",
    "                            dists.append(np.nan)\n",
    "                    dists = np.array(dists)\n",
    "                    dists = dists[np.isfinite(dists)]\n",
    "\n",
    "                    true_trans = euclidean(positions[recalls[ridx]], positions[recalls[ridx+1]])\n",
    "                    pctrank = 1.-percentileofscore(dists, true_trans, kind='strict')/100.\n",
    "                    all_pcts.append(pctrank)\n",
    "\n",
    "                    positions[recalls[ridx]] = np.nan\n",
    "\n",
    "                return np.mean(all_pcts)\n",
    "\n",
    "            def remove_repeats(recalls):\n",
    "                #Takes array of serial positions and remove second instance of a repeated word\n",
    "                items_to_keep = np.ones(len(recalls)).astype(bool)\n",
    "                items_seen = []\n",
    "                idx_removed = []\n",
    "                for idx in range(len(recalls)):\n",
    "                    if recalls[idx] in items_seen:\n",
    "                        items_to_keep[idx] = False\n",
    "                        idx_removed.append(idx)\n",
    "                    items_seen.append(recalls[idx])\n",
    "\n",
    "                final_vec = np.array(recalls)[items_to_keep]\n",
    "                return final_vec, idx_removed\n",
    "            \n",
    "            def get_session_model(words, ndim):\n",
    "    \n",
    "                #Get PCA dims for *session-level* wordpool\n",
    "                words = np.array(words)\n",
    "                if 'AXE' in words:\n",
    "                    words[words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "                word_mat = np.array([model[w] for w in words])\n",
    "                pca = PCA(n_components=ndim)\n",
    "                pcs = pca.fit_transform(word_mat)\n",
    "                exp_var = pca.explained_variance_ratio_\n",
    "                new_model = {}\n",
    "                for idx, w in enumerate(words):\n",
    "                    new_model[w] = pcs[idx, :]\n",
    "\n",
    "                return new_model, exp_var\n",
    "\n",
    "            def get_session_PCs(word_evs, new_model, listnum):\n",
    "\n",
    "                list_dat = word_evs[word_evs['list']==listnum]\n",
    "                list_words = np.array(list_dat['item_name'])\n",
    "                if 'AXE' in list_words:\n",
    "                    list_words[list_words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "\n",
    "                #Get semantic positions from new_model\n",
    "                pcs = np.array([new_model[w_] for w_ in list_words])\n",
    "\n",
    "                return pcs\n",
    "\n",
    "            model = pk.load(open('/home1/esolo/notebooks/Semantic_dimensions/wordpool_feats.pk', 'rb'))\n",
    "            df = get_data_index(\"r1\")\n",
    "\n",
    "            try:\n",
    "\n",
    "                #Load subjects information\n",
    "                s = arg[0]\n",
    "                exp = arg[1]\n",
    "                sess = arg[2]\n",
    "                loc = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['localization'])\n",
    "                mont = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['montage'])\n",
    "\n",
    "                sessions = df[np.logical_and(df[\"subject\"] == s, df['experiment']==exp)]['session'].unique()\n",
    "\n",
    "                #Get task eveents\n",
    "                reader = CMLReader(s, exp, sess, montage=mont, localization=loc)\n",
    "                evs = reader.load(\"events\")\n",
    "                word_evs = evs[evs['type']=='WORD']\n",
    "                \n",
    "                #new_model, exp_var = get_session_model(np.array(word_evs['item_name']), ndim)\n",
    "\n",
    "                all_trans = []\n",
    "                all_temp = []\n",
    "                all_sem = []\n",
    "                \n",
    "                for i in range(251):\n",
    "                    list_sem_sc = []\n",
    "                    list_temp_sc = []\n",
    "                    words_done = []\n",
    "                    \n",
    "                    all_expVar = []\n",
    "                    for listnum in word_evs['list'].unique():\n",
    "                        try:\n",
    "                            #Get info from one list\n",
    "                            list_dat = word_evs[word_evs['list']==listnum]\n",
    "                            words = np.array(list_dat['item_name'])\n",
    "                            if 'AXE' in words:\n",
    "                                words[words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "\n",
    "                            #Project semantic features for this list to 1 dimension\n",
    "                            feats = np.array([model[w] for w in words])  #construct feature matrix from one list\n",
    "                            pca = PCA(n_components=ndim)\n",
    "                            pcs = pca.fit_transform(feats)\n",
    "\n",
    "                            #pcs = get_session_PCs(word_evs, new_model, listnum)\n",
    "\n",
    "                            #Get recall events and their semantic values \n",
    "                            rec_evs = evs[(evs['type']=='REC_WORD') & (evs['list']==listnum) & (evs['intrusion']==0)]\n",
    "                            if len(rec_evs)<4: #don't use lists with fewer than N recalls\n",
    "                                continue\n",
    "                            serial_pos = [int(list_dat[list_dat['item_name']==w]['serialpos'])-1 for w in rec_evs['item_name']]\n",
    "                            serial_pos, repeats_removed = remove_repeats(serial_pos)\n",
    "\n",
    "                            #Get temporal and semantic clustering scores\n",
    "\n",
    "                            #Semantic clustering, randomly draw same number of recalls\n",
    "                            sem_sc = []\n",
    "                            #foo = np.arange(12)\n",
    "                            foo = copy(serial_pos)\n",
    "                            if i == 0: \n",
    "                                list_sem_sc.append(get_recall_clustering(pcs, serial_pos))\n",
    "                            else:\n",
    "                                np.random.shuffle(foo)\n",
    "                                #tmp = foo[:len(serial_pos)]\n",
    "                                tmp = foo\n",
    "                                list_sem_sc.append(get_recall_clustering(pcs, tmp))\n",
    "\n",
    "                            #Temporal clustering, shuffle the actual recall order\n",
    "#                             temp_sc = []\n",
    "#                             foo = copy(serial_pos)\n",
    "#                             if i == 0:\n",
    "#                                 list_temp_sc.append(get_recall_clustering(np.arange(len(pcs)), serial_pos))\n",
    "#                             else:\n",
    "#                                 np.random.shuffle(foo)\n",
    "#                                 tmp = foo\n",
    "#                                 list_temp_sc.append(get_recall_clustering(np.arange(len(pcs)), tmp))\n",
    "\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                    #all_temp.append(list_temp_sc)\n",
    "                    all_sem.append(list_sem_sc)\n",
    "                    print(i)\n",
    "\n",
    "                #Create new directories if needed\n",
    "                try:\n",
    "                    os.mkdir('/scratch/esolo/Semantic_dimensions/'+s+'/')\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    os.mkdir('/scratch/esolo/Semantic_dimensions/'+s+'/'+str(sess)+'/')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                #Save full output\n",
    "                #np.save('/scratch/esolo/Semantic_dimensions/'+s+'/'+str(sess)+'/temporal_clustering.npy', np.array(all_temp))\n",
    "                np.save('/scratch/esolo/Semantic_dimensions/'+s+'/'+str(sess)+'/semantic_clustering_'+str(ndim)+'dims_min4recalls_list_altZscore.npy', np.array(all_sem)) \n",
    "            except:\n",
    "                return\n",
    "            \n",
    "        get_clustering_scores(arg)\n",
    "    \n",
    "\n",
    "import cluster_helper.cluster\n",
    "with cluster_helper.cluster.cluster_view(scheduler=\"sge\", queue=\"RAM.q\", num_jobs=300, cores_per_job=1) as view:\n",
    "    view.map(dims_par_clustering, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 Engines running\n",
      "Sending a shutdown signal to the controller and engines.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 16] Device or resource busy: '.nfs0000001a00dc700a0000001b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8bbe0fbb13c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mcluster_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sge\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RAM.q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcores_per_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims_par_clustering_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mcluster_view\u001b[0;34m(scheduler, queue, num_jobs, cores_per_job, profile, start_wait, extra_params, retries, direct, wait_for_all_engines)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mcluster_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mcluster_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_nengines_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_throwaway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m                 \u001b[0mdelete_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mdelete_profile\u001b[0;34m(profile)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_to_remove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                         \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m _use_fd_functions = ({os.open, os.stat, os.unlink, os.rmdir} <=\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 16] Device or resource busy: '.nfs0000001a00dc700a0000001b'"
     ]
    }
   ],
   "source": [
    "def dims_par_clustering_matrix(arg):\n",
    "\n",
    "    def get_clustering_scores(arg):\n",
    "\n",
    "        import numpy as np\n",
    "        import pickle as pk\n",
    "        from copy import copy\n",
    "        from cmlreaders import CMLReader, get_data_index\n",
    "        import itertools\n",
    "        from scipy.stats import zscore\n",
    "        import os\n",
    "        \n",
    "        def get_recall_clustering_matrix(positions, recalls):\n",
    "            from scipy.stats import percentileofscore\n",
    "            import itertools\n",
    "\n",
    "            #Get semantic clustering scores using a pre-defined matrix of distances (e.g. WordNet)\n",
    "\n",
    "            #Positions: matrix of word-word distances (NxN). Should already be symmetrized.\n",
    "            #Recalls: array of indices for true recall sequence (zero indexed), e.g. [0, 2, 3, 5, 9, 6]\n",
    "\n",
    "            all_pcts = []\n",
    "            all_possible_trans = list(itertools.combinations(range(positions.shape[0]), 2))\n",
    "            for ridx in np.arange(len(recalls)-1):  #Loops through each recall event, except last one\n",
    "                possible_trans = [comb for comb in all_possible_trans if (recalls[ridx] in comb)]\n",
    "                dists = []\n",
    "                for c in possible_trans:\n",
    "                    dists.append(positions[c[0], c[1]])   #could be appending a NaN, but that's okay\n",
    "                dists = np.array(dists)\n",
    "                dists = dists[np.isfinite(dists)]\n",
    "\n",
    "                true_trans = positions[recalls[ridx], recalls[ridx+1]]\n",
    "                pctrank = 1.-percentileofscore(dists, true_trans, kind='strict')/100.\n",
    "                all_pcts.append(pctrank)\n",
    "\n",
    "                positions[recalls[ridx], :] = np.nan; positions[:, recalls[ridx]] = np.nan  #NaN out cols/rows for recalled word\n",
    "\n",
    "            return np.mean(all_pcts)\n",
    "\n",
    "        def remove_repeats(recalls):\n",
    "            #Takes array of serial positions and remove second instance of a repeated word\n",
    "            items_to_keep = np.ones(len(recalls)).astype(bool)\n",
    "            items_seen = []\n",
    "            idx_removed = []\n",
    "            for idx in range(len(recalls)):\n",
    "                if recalls[idx] in items_seen:\n",
    "                    items_to_keep[idx] = False\n",
    "                    idx_removed.append(idx)\n",
    "                items_seen.append(recalls[idx])\n",
    "\n",
    "            final_vec = np.array(recalls)[items_to_keep]\n",
    "            return final_vec, idx_removed\n",
    "\n",
    "        def create_position_matrix(word_list):\n",
    "            from nltk.corpus import wordnet as wn\n",
    "\n",
    "            mat = np.empty([len(word_list), len(word_list)])\n",
    "            mat[:] = np.nan\n",
    "\n",
    "            for idx1, w1 in enumerate(word_list):\n",
    "                for idx2, w2 in enumerate(word_list):\n",
    "                    w1_str = w1.lower()\n",
    "                    w2_str = w2.lower()\n",
    "                    word1 = wn.synset(w1_str+'.n.01')\n",
    "                    word2 = wn.synset(w2_str+'.n.01')\n",
    "\n",
    "                    sim = word1.wup_similarity(word2)\n",
    "                    mat[idx1, idx2] = sim\n",
    "\n",
    "            return mat\n",
    "\n",
    "        df = get_data_index(\"r1\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            #Load subjects information\n",
    "            s = arg[0]\n",
    "            exp = arg[1]\n",
    "            sess = arg[2]\n",
    "            loc = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['localization'])\n",
    "            mont = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['montage'])\n",
    "\n",
    "            sessions = df[np.logical_and(df[\"subject\"] == s, df['experiment']==exp)]['session'].unique()\n",
    "\n",
    "            #Get task eveents\n",
    "            reader = CMLReader(s, exp, sess, montage=mont, localization=loc)\n",
    "            evs = reader.load(\"events\")\n",
    "            word_evs = evs[evs['type']=='WORD']\n",
    "\n",
    "            all_trans = []\n",
    "            all_temp = []\n",
    "            all_sem = []\n",
    "\n",
    "            for i in range(251):\n",
    "                list_sem_sc = []\n",
    "                list_temp_sc = []\n",
    "\n",
    "                all_expVar = []\n",
    "                for listnum in word_evs['list'].unique():\n",
    "                    try:\n",
    "                        #Get info from one list\n",
    "                        list_dat = word_evs[word_evs['list']==listnum]\n",
    "                        words = np.array(list_dat['item_name'])\n",
    "                        if 'AXE' in words:\n",
    "                            words[words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "\n",
    "                        pmat = create_position_matrix(words)\n",
    "\n",
    "                        #Get recall events and their semantic values \n",
    "                        rec_evs = evs[(evs['type']=='REC_WORD') & (evs['list']==listnum) & (evs['intrusion']==0)]\n",
    "                        if len(rec_evs)<4: #don't use lists with fewer than N recalls\n",
    "                            continue\n",
    "                        serial_pos = [int(list_dat[list_dat['item_name']==w]['serialpos'])-1 for w in rec_evs['item_name']]\n",
    "                        serial_pos, repeats_removed = remove_repeats(serial_pos)\n",
    "\n",
    "                        #Get temporal and semantic clustering scores\n",
    "\n",
    "                        #Semantic clustering, randomly draw same number of recalls\n",
    "                        sem_sc = []\n",
    "                        foo = np.arange(12)\n",
    "                        if i == 0: \n",
    "                            list_sem_sc.append(get_recall_clustering_matrix(pmat, serial_pos))\n",
    "                        else:\n",
    "                            np.random.shuffle(foo)\n",
    "                            tmp = foo[:len(serial_pos)]\n",
    "                            list_sem_sc.append(get_recall_clustering_matrix(pmat, tmp))\n",
    "\n",
    "    #                         #Temporal clustering, shuffle the actual recall order\n",
    "    #                         temp_sc = []\n",
    "    #                         foo = copy(serial_pos)\n",
    "    #                         if i == 0:\n",
    "    #                             list_temp_sc.append(get_recall_clustering(np.arange(len(pcs)), serial_pos))\n",
    "    #                         else:\n",
    "    #                             np.random.shuffle(foo)\n",
    "    #                             tmp = foo\n",
    "    #                             list_temp_sc.append(get_recall_clustering(np.arange(len(pcs)), tmp))\n",
    "\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "                #all_temp.append(list_temp_sc)\n",
    "                all_sem.append(list_sem_sc)\n",
    "                print(i)\n",
    "\n",
    "            #Create new directories if needed\n",
    "            try:\n",
    "                os.mkdir('/scratch/esolo/grids/'+s+'/')\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                os.mkdir('/scratch/esolo/grids/'+s+'/'+str(sess)+'/')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            #Save full output\n",
    "            #np.save('/scratch/esolo/Semantic_dimensions/'+s+'/'+str(sess)+'/temporal_clustering.npy', np.array(all_temp))\n",
    "            np.save('/scratch/esolo/Semantic_dimensions/'+s+'/'+str(sess)+'/semantic_clustering_min4recalls_wordNet.npy', np.array(all_sem)) \n",
    "        except:\n",
    "            return\n",
    "\n",
    "    get_clustering_scores(arg)\n",
    "    \n",
    "\n",
    "import cluster_helper.cluster\n",
    "with cluster_helper.cluster.cluster_view(scheduler=\"sge\", queue=\"RAM.q\", num_jobs=300, cores_per_job=1) as view:\n",
    "    view.map(dims_par_clustering_matrix, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 Engines running\n",
      "Sending a shutdown signal to the controller and engines.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 16] Device or resource busy: '.nfs0000000b018a401900000077'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-ac8b63b4cc58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mcluster_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sge\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RAM.q\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcores_per_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims_par_clustering_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mcluster_view\u001b[0;34m(scheduler, queue, num_jobs, cores_per_job, profile, start_wait, extra_params, retries, direct, wait_for_all_engines)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mcluster_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mcluster_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_nengines_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_throwaway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m                 \u001b[0mdelete_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/site-packages/ipython_cluster_helper-0.6.1-py3.6.egg/cluster_helper/cluster.py\u001b[0m in \u001b[0;36mdelete_profile\u001b[0;34m(profile)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_to_remove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m                         \u001b[0m_rmtree_safe_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m _use_fd_functions = ({os.open, os.stat, os.unlink, os.rmdir} <=\n",
      "\u001b[0;32m~/anaconda3/envs/CML/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0monerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 16] Device or resource busy: '.nfs0000000b018a401900000077'"
     ]
    }
   ],
   "source": [
    "def dims_par_clustering_session(arg):\n",
    "\n",
    "    for d_ in range(14, 25):\n",
    "\n",
    "        def get_clustering_scores(arg, ndim=d_):\n",
    "\n",
    "            import numpy as np\n",
    "            import pickle as pk\n",
    "            from copy import copy\n",
    "            from cmlreaders import CMLReader, get_data_index\n",
    "            import itertools\n",
    "            from sklearn.decomposition import PCA\n",
    "            from scipy.stats import zscore\n",
    "            from scipy.spatial.distance import euclidean\n",
    "            import os\n",
    "\n",
    "            def get_recall_clustering(positions, recalls):\n",
    "                from scipy.stats import percentileofscore\n",
    "                #Get temporal/semantic clustering scores. \n",
    "\n",
    "                #Positions: array of semantic/temporal values\n",
    "                #Recalls: array of indices for true recall sequence (zero indexed), e.g. [0, 2, 3, 5, 9, 6]\n",
    "\n",
    "                positions = copy(np.array(positions).astype(float))\n",
    "                all_pcts = []\n",
    "                all_possible_trans = list(itertools.combinations(range(len(positions)), 2))\n",
    "                for ridx in np.arange(len(recalls)-1):  #Loops through each recall event, except last one\n",
    "                    possible_trans = [comb for comb in all_possible_trans if (recalls[ridx] in comb)]\n",
    "                    dists = []\n",
    "                    for c in possible_trans:\n",
    "                        try:\n",
    "                            dists.append(euclidean(positions[c[0]], positions[c[1]]))\n",
    "                        except:\n",
    "                            #If we did this transition, then it's a NaN, so append a NaN\n",
    "                            dists.append(np.nan)\n",
    "                    dists = np.array(dists)\n",
    "                    dists = dists[np.isfinite(dists)]\n",
    "\n",
    "                    true_trans = euclidean(positions[recalls[ridx]], positions[recalls[ridx+1]])\n",
    "                    pctrank = 1.-percentileofscore(dists, true_trans)/100.\n",
    "                    all_pcts.append(pctrank)\n",
    "\n",
    "                    positions[recalls[ridx]] = np.nan\n",
    "\n",
    "                return np.mean(all_pcts)\n",
    "\n",
    "            def remove_repeats(recalls):\n",
    "                #Takes array of serial positions and remove second instance of a repeated word\n",
    "                items_to_keep = np.ones(len(recalls)).astype(bool)\n",
    "                items_seen = []\n",
    "                idx_removed = []\n",
    "                for idx in range(len(recalls)):\n",
    "                    if recalls[idx] in items_seen:\n",
    "                        items_to_keep[idx] = False\n",
    "                        idx_removed.append(idx)\n",
    "                    items_seen.append(recalls[idx])\n",
    "\n",
    "                final_vec = np.array(recalls)[items_to_keep]\n",
    "                return final_vec, idx_removed\n",
    "\n",
    "            model = pk.load(open('/home1/esolo/notebooks/Semantic_dimensions/wordpool_feats.pk', 'rb'))\n",
    "            df = get_data_index(\"r1\")\n",
    "\n",
    "            try:\n",
    "\n",
    "                #Load subjects information\n",
    "                s = arg[0]\n",
    "                exp = arg[1]\n",
    "                sess = arg[2]\n",
    "                loc = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['localization'])\n",
    "                mont = int(df[(df['subject']==s) & (df['session']==sess) & (df['experiment']==exp)]['montage'])\n",
    "\n",
    "                sessions = df[np.logical_and(df[\"subject\"] == s, df['experiment']==exp)]['session'].unique()\n",
    "\n",
    "                #Get task eveents\n",
    "                reader = CMLReader(s, exp, sess, montage=mont, localization=loc)\n",
    "                evs = reader.load(\"events\")\n",
    "                word_evs = evs[evs['type']=='WORD']\n",
    "\n",
    "                all_trans = []\n",
    "                all_temp = []\n",
    "                all_sem = []\n",
    "                \n",
    "                #Get PCA dims for *session-level* wordpool\n",
    "                words = np.array(word_evs['item_name'])\n",
    "                if 'AXE' in words:\n",
    "                    words[words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "                word_mat = np.array([model[w] for w in words])\n",
    "                pca = PCA(n_components=ndim)\n",
    "                pcs = pca.fit_transform(word_mat)\n",
    "                new_model = {}\n",
    "                for idx, w in enumerate(word_evs['item_name']):\n",
    "                    new_model[w] = pcs[idx, :]\n",
    "\n",
    "                for i in range(251):\n",
    "                    list_sem_sc = []\n",
    "                    list_temp_sc = []\n",
    "\n",
    "                    for listnum in word_evs['list'].unique():\n",
    "                        try:\n",
    "\n",
    "                            list_dat = word_evs[word_evs['list']==listnum]\n",
    "                            list_words = np.array(list_dat['item_name'])\n",
    "                            if 'AXE' in list_words:\n",
    "                                list_words[list_words=='AXE']='AX'  #seems to not have this spelling of ax\n",
    "                            \n",
    "                            #Get recall events and their semantic values from one list\n",
    "                            rec_evs = evs[(evs['type']=='REC_WORD') & (evs['list']==listnum) & (evs['intrusion']==0)]\n",
    "                            if len(rec_evs)<3: #don't use lists with fewer than 3 recalls\n",
    "                                continue\n",
    "                            serial_pos = [int(list_dat[list_dat['item_name']==w]['serialpos'])-1 for w in rec_evs['item_name']]\n",
    "                            serial_pos, repeats_removed = remove_repeats(serial_pos)\n",
    "\n",
    "                            #Get semantic positions from new_model\n",
    "                            pcs = np.array([new_model[w_] for w_ in list_words])\n",
    "\n",
    "                            #Semantic clustering, randomly draw same number of recalls\n",
    "                            sem_sc = []\n",
    "                            foo = np.arange(12)\n",
    "                            if i == 0: \n",
    "                                list_sem_sc.append(get_recall_clustering(pcs, serial_pos))\n",
    "                            else:\n",
    "                                np.random.shuffle(foo)\n",
    "                                tmp = foo[:len(serial_pos)]\n",
    "                                list_sem_sc.append(get_recall_clustering(pcs, tmp))\n",
    "\n",
    "                            #Temporal clustering, shuffle the actual recall order\n",
    "        #                     temp_sc = []\n",
    "        #                     foo = copy(serial_pos)\n",
    "        #                     if i == 0:\n",
    "        #                         list_temp_sc.append(get_recall_clustering(np.arange(len(pcs)), serial_pos))\n",
    "        #                     else:\n",
    "        #                         np.random.shuffle(foo)\n",
    "        #                         tmp = foo\n",
    "        #                         list_temp_sc.append(get_recall_clustering(np.arange(len(pcs)), tmp))\n",
    "\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                    all_temp.append(list_temp_sc)\n",
    "                    all_sem.append(list_sem_sc)\n",
    "                    print(i)\n",
    "\n",
    "                #Create new directories if needed\n",
    "                try:\n",
    "                    os.mkdir('/scratch/esolo/Semantic_dimensions/'+s+'/')\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    os.mkdir('/scratch/esolo/Semantic_dimensions/'+s+'/'+str(sess)+'/')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                #Save full output\n",
    "                #np.save('/scratch/esolo/grids/'+s+'/'+str(sess)+'/temporal_clustering.npy', np.array(all_temp))\n",
    "                np.save('/scratch/esolo/Semantic_dimensions/'+s+'/'+str(sess)+'/semantic_clustering_'+str(ndim)+'dims_min3recalls_session.npy', np.array(all_sem)) \n",
    "            except:\n",
    "                return\n",
    "            \n",
    "        get_clustering_scores(arg)\n",
    "    \n",
    "\n",
    "import cluster_helper.cluster\n",
    "with cluster_helper.cluster.cluster_view(scheduler=\"sge\", queue=\"RAM.q\", num_jobs=300, cores_per_job=1) as view:\n",
    "    view.map(dims_par_clustering_session, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
